cleanup_eval_on_failure: false
dataset_configs:
- datasets/sick.yaml
description: "\n# Overview\n## Task Description\nThis is a Machine Learning project\
  \ and your goal is to complete the project's TASK following the instructions provided\
  \ below.\n\n    TASK: This is a NLP task to perform textual-relatedness on RobZamp/sick.\
  \ You will be provided with a pair of sentences, `sentence_A` and `sentence_B`,\
  \ and will need to predict a similarity score between 0 (low) and 5 (high) indicating\
  \ the semantic relatedness of the two sentences. Your predictions will be scored\
  \ against the `relatedness_score` column of the test set.\n\n## Data\n### Dataset\
  \ Structure\nThe default config of RobZamp/sick has the following structure.\nHere\
  \ is a description of the contents of each column including their name, what they\
  \ contain, and the data type:\n ``` \n    {{\n        'relatedness_score': Value('float64'),\
  \ #\_similarity score between 0 and 5 (This is the scoring column to predict)\n\
  \        'sentence_A': Value('string'), # first sentence of the sentence pair\n\
  \        'sentence_B': Value('string') # second sentence of the sentence pair \n\
  \    }} \n```\nTrain data can be loaded using:\n```python\n    from datasets import\
  \ load_from_disk\n    load_from_disk('./data/train') \n```\nValidation data can\
  \ be loaded using:\n```python\n    from datasets import load_from_disk\n    load_from_disk('./data/validation')\n\
  ```\nTest data can be loaded using:\n```python\n    from datasets import load_from_disk\n\
  \    load_from_disk('./data/test').\n```\nNote that the scoring column has been\
  \ removed from the test data.\n### Submission file\nThe submission file should be\
  \ a csv file named `submission.csv` with the following header:\n``` relatedness_score\
  \ ```\nWhere each row contains your predicted similarity score (a float between\
  \ 0 and 5) for the corresponding row in the test set.\nAnd it should be of shape\
  \ (4906,).\n\n### Evalution\nThe evaluation will be performed on the `submission.csv`\
  \ file you have submitted using the SpearmanCorrelation metric. Here is the evaluation\
  \ script that will be used:\n```python\nfrom scipy.stats import spearmanr\n\ndef\
  \ evaluate(predictions, labels):\n    \"\"\"\n    Returns a dict of metric_name\
  \ -> value\n    \"\"\"\n    return {{\"SpearmanCorrelation\": spearmanr(predictions,\
  \ labels).correlation}}\n```\n\nYour csv submission will be loaded like this:\n\
  ```python\nimport pandas as pd\nsubmission_df = pd.read_csv(a.submission_file, header=0)\n\
  preds = submission_df.values.squeeze()\n```\n    If a baseline is given, your task\
  \ is to train a new model that improves performance on the given dataset as much\
  \ as possible. If you fail to produce a valid submission artefact evaluation file\
  \ will give you a score of 0.\n{dataset_docs}"
eval_timeout: 3600
evaluation_paths:
- evaluate.py
evaluation_read_only: true
id: TextualSimilaritySickSpearmanCorrelation
memory_path: data/TextualSimilaritySickSpearmanCorrelation/memory.json
metric_lower_is_better: false
name: TextualSimilaritySickSpearmanCorrelation
requirements_path: data/TextualSimilaritySickSpearmanCorrelation/requirements.txt
starter_code:
- data/TextualSimilaritySickSpearmanCorrelation/evaluate.py
task_entrypoint: CSVSubmissionTasks
training_timeout: 3600
use_generic_conda: false
use_separate_eval_container: true
