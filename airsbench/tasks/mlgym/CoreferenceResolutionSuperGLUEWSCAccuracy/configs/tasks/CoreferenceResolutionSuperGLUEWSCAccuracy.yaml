dataset_configs:
- datasets/super_glue.yaml
description: "# Overview\n## Task Description\nThis is a Machine Learning project\
  \ and your goal is to build a model that solves the project's TASK following the\
  \ instructions provided below.\n\nTASK: Your task is to resolve pronoun references\
  \ in natural language sentences. You will be given a sentence containing an ambiguous\
  \ pronoun and a possible referent. Your goal is to predict whether the pronoun refers\
  \ to the referent based on commonsense reasoning. Your predictions will be scored\
  \ against the `label` column of the test.\n\n## Data\n### Dataset Structure\nThe\
  \ default config of the WSC dataset has the following structure.\nHere is a description\
  \ of the contents of each column including their name, what they contain, and the\
  \ data type:\n```\n{{\n    \"text\": string, # The passage containing an ambiguous\
  \ pronoun and candidate antecedent\n    \"span1_index\": int, # The token index\
  \ of the first candidate antecedent in the passage\n    \"span2_index\": int, #\
  \ The token index of the pronoun in the passage\n    \"span1_text\": string, # The\
  \ text span corresponding to the candidate antecedent\n    \"span2_text\": string,\
  \ # The text span corresponding to the pronoun\n    \"label\": int # Binary label:\
  \ 1 if the pronoun refers to the antecedent, 0 otherwise\n}}\n```\n\nAn example\
  \ entry in the train dataset contains the following:\n- `text`: \"Mark told Pete\
  \ many lies about himself, which Pete included in his book. He should have been\
  \ more skeptical.\"\n- `span1_index`: 0\n- `span2_index`: 13\n- `span1_text`: \"\
  Mark\"\n- `span2_text`: \"He\"\n- `label`: 0\n\n> Note: The task is to determine\
  \ whether the pronoun (`span2_text`) refers to the antecedent candidate (`span1_text`).\
  \ A label of `1` indicates coreference, while `0` indicates no coreference. The\
  \ `label` column is only available in the test set. Ensure you preserve the original\
  \ row order of the test set when producing predictions.\n\n\nTrain data can be loaded\
  \ using:\n```python\nfrom datasets import load_from_disk\ntrain_dataset = load_from_disk('./data/train')\n\
  ```\n\nValidation data can be loaded using:\n```python\nfrom datasets import load_from_disk\n\
  test_dataset = load_from_disk('./data/validation')\n```\n\nTest data can be loaded\
  \ using:\n```python\nfrom datasets import load_from_disk\ntest_dataset = load_from_disk('./data/test')\n\
  ```\n\n\n### Submission file\nThe submission file should contain the selected answers\
  \ for the test set. The submission file should be a csv file named `submission.csv`\
  \ with the following header:\n```\nAnswer\n```\n\nAnd it should be of shape `(104,1)`\
  \ \u2014 one prediction per row, aligned with the test set order.\n\nEach row must\
  \ contain `1` pronoun (`span2_text`) refers to the antecedent candidate (`span1_text`),\
  \ and `0` otherwise. Do not include any extra columns or indices.\n\n### Evalution\n\
  The evaluation will be performed on the `submission.csv` file you have submitted\
  \ using the Accuracy metric.\n\nEvaluation will be done using the following script:\n\
  \n```py\n#!/usr/bin/env python3\nimport argparse, json\nimport numpy as np\nimport\
  \ pandas as pd\nfrom datasets import load_from_disk\n\n\ndef load_test_answers(path='./data/test_with_labels'):\n\
  \    \"\"\"\n    Load answers for evaluation (string labels).\n    \"\"\"\n    ds\
  \ = load_from_disk(path)\n    if \"label\" not in ds.features:\n        raise RuntimeError(\n\
  \            \"Test data does not contain 'label'. \"\n            \"Use a labeled\
  \ copy like './data/test_with_labels'.\"\n        )\n    labels = list(ds[\"label\"\
  ])\n    return labels\n\n\ndef evaluate(predictions, labels):\n    \"\"\"\n    Returns\
  \ a dict of metric_name -> value\n    \"\"\"\n    correct = np.fromiter(\n     \
  \   (int(p) == int(y) for p, y in zip(predictions, labels)), dtype=bool\n    )\n\
  \    return {{\"Accuracy\": float(correct.mean())}}\n\n\ndef _cli():\n    p = argparse.ArgumentParser(\n\
  \        description=\"Evaluate predictions for TabFact test split using Accuracy.\"\
  \n    )\n    p.add_argument(\"--submission-file\", default=\"submission.csv\",\n\
  \                   help=\"Path to CSV file containing predictions.\")\n    a =\
  \ p.parse_args()\n\n    print(\"Loading test set labels...\")\n    labels = load_test_answers()\n\
  \    n_test_samples = len(labels)\n    print(f\"Loaded {{n_test_samples}} labels.\"\
  )\n\n    print(f\"Loading predictions from: {{a.submission_file}}\")\n    try:\n\
  \        # Assuming CSV has no header and contains only prediction values\n    \
  \    # Adjust if your submission format is different (e.g., has headers, specific\
  \ columns)\n        submission_df = pd.read_csv(a.submission_file, header=0)\n \
  \       preds = submission_df.values.squeeze()\n        if preds.shape[0] != n_test_samples:\n\
  \            raise ValueError(\n                f\"Submission file row count ({{preds.shape[0]}})\
  \ \"\n                f\"does not match test set size ({{n_test_samples}}).\"\n\
  \            )\n    except FileNotFoundError:\n        p.error(f\"Submission file\
  \ not found: {{a.submission_file}}\")\n    except Exception as e:\n        p.error(f\"\
  Error loading submission_file: {{e}}\")\n\n    print(\"Evaluating predictions...\"\
  )\n    result = evaluate(preds, labels)\n\n    print(\"\\n--- EVALUATION RESULT\
  \ ---\")\n    print(json.dumps(result, indent=2))\n\n\nif __name__ == '__main__':\n\
  \    _cli()\n```If a baseline is given, your task is to train a new model that improves\
  \ performance on the given dataset as much as possible. If you fail to produce a\
  \ valid submission artefact evaluation file will give you a score of 0.\n{dataset_docs}"
evaluation_paths:
- evaluate.py
evaluation_read_only: true
id: CoreferenceResolutionSuperGLUEWSCAccuracy
memory_path: data/CoreferenceResolutionSuperGLUEWSCAccuracy/memory.json
metric_lower_is_better: false
name: CoreferenceResolutionSuperGLUEWSCAccuracy
requirements_path: data/CoreferenceResolutionSuperGLUEWSCAccuracy/requirements.txt
starter_code:
- data/CoreferenceResolutionSuperGLUEWSCAccuracy/evaluate.py
task_entrypoint: CSVSubmissionTasks
training_timeout: 3600
use_generic_conda: false
