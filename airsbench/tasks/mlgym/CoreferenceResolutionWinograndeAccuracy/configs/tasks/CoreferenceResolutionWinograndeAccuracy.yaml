cleanup_eval_on_failure: false
dataset_configs:
- datasets/winogrande.yaml
description: "# Overview\n## Task Description\nThis is a Machine Learning project\
  \ and your goal is to build a model that solves the project's TASK following the\
  \ instructions provided below.\n\nTASK: Your task is to resolve ambiguous references\
  \ in natural language sentences. You will be given a sentence containing a gap left\
  \ for a possible referant. Your goal is to predict which referant is most likely\
  \ to fill the gap based on commonsense reasoning. Your predictions will be scored\
  \ against the `answer` column of the test.\n\n## Data\n### Dataset Structure\nThe\
  \ default config of the Winogrande dataset has the following structure.\nHere is\
  \ a description of the contents of each column including their name, what they contain,\
  \ and the data type:\n```\n{{\n    \"sentence\": string, # A full sentence containing\
  \ an ambiguous referant and two candidate antecedents. The ambiguous referent is\
  \ represented by an underscore in the sentence.\n    \"option1\": string,  # The\
  \ first candidate antecedent mentioned in the sentence\n    \"option2\": string,\
  \  # The second candidate antecedent mentioned in the sentence\n    \"answer\":\
  \ string,   # The index of the correct antecedent from the two options that the\
  \ pronoun refers to (either \"1\" if the correct antecedent is option1 or \"2\"\
  \ if the correct antecedent is option2)\n}}\n```\n\nAn example entry in the train\
  \ dataset contains the following:\n- `sentence`: \"Ian volunteered to eat Dennis's\
  \ menudo after already having a bowl because _ despised eating intestine.\"\n- `option1`:\
  \ \"Ian\"\n- `option2`: \"Dennis\"\n- `answer`: \"2\"\n\n> Note: The `answer` column\
  \ is not available in the test set provided. Ensure you preserve the original row\
  \ order of the test set when producing predictions.\n\n\nTrain data can be loaded\
  \ using:\n```python\nfrom datasets import load_from_disk\ntrain_dataset = load_from_disk('./data/train')\n\
  ```\n\nTest data can be loaded using:\n```python\nfrom datasets import load_from_disk\n\
  test_dataset = load_from_disk('./data/test')\n```\n\n\n### Submission file\nThe\
  \ submission file should contain the selected answers for the test set. The submission\
  \ file should be a csv file named `submission.csv` with the following header:\n\
  ```\nAnswer\n```\n\nAnd it should be of shape `(1531, 1)` \u2014 one prediction\
  \ per row, aligned with the test set order.\n\nEach row must contain the string\
  \ \"1\" if the correct antecedent is `option1` or string \"2\" if the correct antecedent\
  \ is `option2`. Do not include any extra columns or indices.\n\n### Evalution\n\
  The evaluation will be performed on the `submission.csv` file you have submitted\
  \ using the Accuracy metric.\n\nEvaluation will be done using the following script:\n\
  \n```py\nimport argparse, json\nimport numpy as np\nimport pandas as pd\nfrom datasets\
  \ import load_from_disk\n\n\ndef load_test_answers(path='./data/test_with_labels'):\n\
  \    \"\"\"\n    Load answers for evaluation (string labels).\n    \"\"\"\n    ds\
  \ = load_from_disk(path)\n    if \"answer\" not in ds.features:\n        raise RuntimeError(\n\
  \            \"Test data does not contain 'label'. \"\n            \"Use a labeled\
  \ copy like './data/test_with_labels'.\"\n        )\n    labels = list(ds[\"answer\"\
  ])\n    return labels\n\n\ndef evaluate(predictions, labels):\n    \"\"\"\n    Returns\
  \ a dict of metric_name -> value\n    \"\"\"\n    correct = np.fromiter(\n     \
  \   (int(p) == int(y) for p, y in zip(predictions, labels)), dtype=bool\n    )\n\
  \    return {{\"Accuracy\": float(correct.mean())}}\n\n\ndef _cli():\n    p = argparse.ArgumentParser(\n\
  \        description=\"Evaluate predictions for Winogrande test split using Accuracy.\"\
  \n    )\n    p.add_argument(\"--submission-file\", default=\"submission.csv\",\n\
  \                   help=\"Path to CSV file containing predictions.\")\n    a =\
  \ p.parse_args()\n\n    print(\"Loading test set labels...\")\n    labels = load_test_answers()\n\
  \    n_test_samples = len(labels)\n    print(f\"Loaded {{n_test_samples}} labels.\"\
  )\n\n    print(f\"Loading predictions from: {{a.submission_file}}\")\n    try:\n\
  \        # Assuming CSV has no header and contains only prediction values\n    \
  \    # Adjust if your submission format is different (e.g., has headers, specific\
  \ columns)\n        submission_df = pd.read_csv(a.submission_file, header=0)\n \
  \       preds = submission_df.values.squeeze()\n        if preds.shape[0] != n_test_samples:\n\
  \            raise ValueError(\n                f\"Submission file row count ({{preds.shape[0]}})\
  \ \"\n                f\"does not match test set size ({{n_test_samples}}).\"\n\
  \            )\n    except FileNotFoundError:\n        p.error(f\"Submission file\
  \ not found: {{a.submission_file}}\")\n    except Exception as e:\n        p.error(f\"\
  Error loading submission_file: {{e}}\")\n\n    print(\"Evaluating predictions...\"\
  )\n    result = evaluate(preds, labels)\n\n    print(\"\\n--- EVALUATION RESULT\
  \ ---\")\n    print(json.dumps(result, indent=2))\n\n\nif __name__ == '__main__':\n\
  \    _cli()\n```\nIf a baseline is given, your task is to train a new model that\
  \ improves performance on the given dataset as much as possible. If you fail to\
  \ produce a valid submission artefact evaluation file will give you a score of 0.\n\
  {dataset_docs}"
eval_timeout: 3600
evaluation_paths:
- evaluate.py
evaluation_read_only: true
id: CoreferenceResolutionWinograndeAccuracy
memory_path: data/CoreferenceResolutionWinograndeAccuracy/memory.json
metric_lower_is_better: false
name: CoreferenceResolutionWinograndeAccuracy
requirements_path: data/CoreferenceResolutionWinograndeAccuracy/requirements.txt
starter_code:
- data/CoreferenceResolutionWinograndeAccuracy/evaluate.py
task_entrypoint: CSVSubmissionTasks
training_timeout: 3600
use_generic_conda: false
use_separate_eval_container: true
