{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e0bf36e9",
      "metadata": {},
      "source": [
        "# Process Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7755126d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from collections import defaultdict\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from pathlib import Path\n",
        "from typing import List, Optional, Tuple, Union\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yaml\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.patches import Patch\n",
        "from scipy.stats import t\n",
        "from tqdm import tqdm\n",
        "import rliable\n",
        "import rliable.library\n",
        "import rliable.metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b53ef1e",
      "metadata": {},
      "source": [
        "### Gather task metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c7e2d13",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_task_metadata(task_name):\n",
        "    \"\"\"\n",
        "    Given a task_name, looks for a folder with the same name under `tasks/rad`,\n",
        "    finds a `metadata.yaml` file under that folder, and returns the parsed content.\n",
        "    If the file does not exist, it raises a FileNotFoundError with additional context.\n",
        "    \"\"\"\n",
        "    current_path = Path.cwd()\n",
        "    current_path = current_path.parent\n",
        "    metadata_file = current_path / \"airsbench/tasks/rad\" / task_name / \"metadata.yaml\"\n",
        "\n",
        "    # if not task_dir.exists():\n",
        "    #     raise FileNotFoundError(f\"Task directory not found: {task_dir}\")\n",
        "\n",
        "    if not metadata_file.is_file():\n",
        "        raise FileNotFoundError(f\"metadata.yaml not found for task: {task_name}\")\n",
        "\n",
        "    return yaml.safe_load(metadata_file.read_text())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a3a623e",
      "metadata": {
        "output": {
          "id": 1632328234561823,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "task_name = \"TimeSeriesForecastingSolarWeeklyMAE\"\n",
        "metadata = get_task_metadata(task_name)\n",
        "pprint(metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1df682c3",
      "metadata": {},
      "source": [
        "## Compile results\n",
        "- `task_metadata` is a dict storing metadata as `task_metadata[task]`. \n",
        "- `full_results` is a dict storing results as `full_results[task][agent]`. Each value is a list storing tuples (score, status)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b91741d4",
      "metadata": {},
      "outputs": [],
      "source": [
        "pkl_files = {\n",
        "    \"mlgym_gpt-4o\": \"/checkpoint/maui/shared/logs/airs_bench_analysis/mlgym_results/analysis_v2_HEURISTIC_20t_airsbench_4o_20260205.pkl\",\n",
        "    \"mlgym_cwm\": \"/checkpoint/maui/shared/logs/airs_bench_analysis/mlgym_results/analysis_v2_HEURISTIC_20t_airsbench_cwm_20260205.pkl\",\n",
        "    \"oneshot_o3_mini\": \"/checkpoint/maui/shared/logs/airs_bench_analysis/results_lite/oneshot_o3_mini__results.pkl\",\n",
        "    \"oneshot_cwm\": \"/checkpoint/maui/shared/logs/airs_bench_analysis/results_lite/oneshot_facebook_cwm__results.pkl\",\n",
        "    \"oneshot_gpt-4o\": \"/checkpoint/maui/shared/logs/airs_bench_analysis/results_lite/oneshot_gpt-4o__results.pkl\",\n",
        "    \"oneshot_gpt-oss-20b\": \"/checkpoint/maui/shared/logs/airs_bench_analysis/results_lite/oneshot_gpt-oss-20b__results.pkl\",\n",
        "    \"oneshot_gpt-oss-120b\": \"/checkpoint/maui/shared/logs/airs_bench_analysis/results_lite/oneshot_gpt-oss-120b__results.pkl\",\n",
        "    \"oneshot_devstral\": \"/checkpoint/maui/shared/logs/airs_bench_analysis/results_lite/oneshot_devstral__results.pkl\",\n",
        "    \"greedy_devstral\": \"/checkpoint/maui/shared/logs/airs_bench_analysis/results_lite/greedy_devstral__results.pkl\",\n",
        "    \"greedy_gpt-oss-20b\": \"/checkpoint/maui/shared/logs/airs_bench_analysis/results_lite/greedy_gpt-oss-20b__results.pkl\",\n",
        "    \"greedy_cwm\": \"/checkpoint/maui/shared/logs/airs_bench_analysis/results_lite/greedy_facebook_cwm__results.pkl\",\n",
        "    \"greedy_gpt-4o\": \"/checkpoint/maui/shared/logs/airs_bench_analysis/results_lite/greedy_gpt-4o__results.pkl\",\n",
        "    \"greedy_o3_mini\": \"/checkpoint/maui/shared/logs/airs_bench_analysis/results_lite/greedy_o3_mini__results.pkl\",\n",
        "    \"greedy_gpt-oss-120b\": \"/checkpoint/maui/shared/logs/airs_bench_analysis/results_lite/greedy_gpt-oss-120b__results.pkl\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb278841",
      "metadata": {
        "output": {
          "id": 1344038421095511,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Load and print the contents of the 'mlgym_cwm' pickle file\n",
        "with open(pkl_files[\"oneshot_o3_mini\"], \"rb\") as f:\n",
        "    oneshot_o3_mini_data = pickle.load(f)\n",
        "\n",
        "print(oneshot_o3_mini_data)\n",
        "\n",
        "with open(pkl_files[\"mlgym_cwm\"], \"rb\") as f:\n",
        "    mlgym_cwm_data = pickle.load(f)\n",
        "\n",
        "print(mlgym_cwm_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea4a9fd7",
      "metadata": {
        "output": {
          "id": 3847682722203315,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "len(mlgym_cwm_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70439787",
      "metadata": {},
      "outputs": [],
      "source": [
        "tasks = [\n",
        "    \"TimeSeriesForecastingSolarWeeklyMAE\",\n",
        "    \"TextualSimilaritySickSpearmanCorrelation\",\n",
        "    \"TextualClassificationSickAccuracy\",\n",
        "    \"CvMolecularPropertyPredictionQm9MeanAbsoluteError\",\n",
        "    \"ReadingComprehensionSquadExactMatch\",\n",
        "    \"GMolecularPropertyPredictionQm9MeanAbsoluteError\",\n",
        "    \"CoreferenceResolutionWinograndeAccuracy\",\n",
        "    \"TimeSeriesForecastingKaggleWebTrafficMASE\",\n",
        "    \"U0MolecularPropertyPredictionQm9MeanAbsoluteError\",\n",
        "    \"R2AbsMolecularPropertyPredictionQm9MeanAbsoluteError\",\n",
        "    \"SentimentAnalysisYelpReviewFullAccuracy\",\n",
        "    \"GraphRegressionZincMae\",\n",
        "    \"CoreferenceResolutionSuperGLUEWSCAccuracy\",\n",
        "    \"QuestionAnsweringEli5Rouge1\",\n",
        "    \"TimeSeriesForecastingRideshareMAE\",\n",
        "    \"QuestionAnsweringDuoRCAccuracy\",\n",
        "    \"CodeGenerationAPPSPassAt5\",\n",
        "    \"CodeRetrievalCodeXGlueMRR\",\n",
        "    \"MathQuestionAnsweringSVAMPAccuracy\",\n",
        "    \"QuestionAnsweringFinqaAccuracy\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dafbfd1",
      "metadata": {
        "output": {
          "id": 1069594192011582,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "parent_dir = os.path.dirname(os.path.abspath(os.getcwd()))\n",
        "sys.path.append(parent_dir)\n",
        "\n",
        "full_results = {}\n",
        "task_metadata = {}\n",
        "\n",
        "for agent, file_path in pkl_files.items():\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        agent_data = pickle.load(f)\n",
        "    print(agent)\n",
        "    for task_name, task_results in agent_data.items():\n",
        "        if task_name not in tasks:\n",
        "            continue\n",
        "        print(task_name)\n",
        "        if task_name not in task_metadata:\n",
        "            task_metadata[task_name] = get_task_metadata(task_name)\n",
        "        # print(task_results)\n",
        "        processed_task_results = []\n",
        "        # print(task_results[\"submitted_solution_metrics\"])\n",
        "        if agent.startswith(\"mlgym\"):\n",
        "            print(f\"Processing MLGym agent: {agent}\")\n",
        "            for results_list in task_results[\"submitted_solution_metrics\"]:\n",
        "                processed_task_results.append((results_list[0], results_list[1]))\n",
        "        else:\n",
        "            print(f\"Processing RAD agent: {agent}\")\n",
        "            for score, status, _ in task_results[\"submitted_solution_metrics\"]:\n",
        "                processed_task_results.append((score, status))\n",
        "\n",
        "        task_bucket = full_results.setdefault(task_name, {})\n",
        "\n",
        "        task_bucket[agent] = processed_task_results\n",
        "        full_results[task_name] = task_bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9f12d4e",
      "metadata": {
        "output": {
          "id": 1580346343175635,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "pprint(full_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbd91966",
      "metadata": {},
      "source": [
        "# Elo Calculation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74bfe638",
      "metadata": {},
      "source": [
        "## Prepare Results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c980a643",
      "metadata": {},
      "source": [
        "### Add SOTA as Player"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1d21dcc",
      "metadata": {
        "output": {
          "id": 752961764176821,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "elo_full_results = deepcopy(full_results)\n",
        "for task in full_results.keys():\n",
        "    sota = task_metadata[task]['logging_info'][\"sota\"][0]['sota_score']\n",
        "    elo_full_results[task][\"sota\"] = [(sota, \"SUCCESS\")]*10"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc5a1997",
      "metadata": {},
      "source": [
        "### Calculate Head to Head Scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "873584fd",
      "metadata": {
        "output": {
          "id": 943842364748437,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "results_lmsys = []\n",
        "for task, task_res in elo_full_results.items():\n",
        "    lower_is_better = bool(task_metadata[task][\"metric_lower_is_better\"])\n",
        "    methods = list(task_res.keys())\n",
        "    for m1 in range(len(methods)):\n",
        "        model_a = methods[m1]\n",
        "        for m2 in range(m1+1, len(methods)):\n",
        "            model_b = methods[m2]\n",
        "            scores_1 = task_res[methods[m1]]\n",
        "            scores_2 = task_res[methods[m2]]\n",
        "\n",
        "\n",
        "            for s1 in scores_1:\n",
        "                val_1 = s1[0]\n",
        "                for s2 in scores_2:\n",
        "                    val_2 = s2[0]\n",
        "                    info = {\"model_a\": model_a, \"model_b\": model_b}\n",
        "                    # --- Handle N/A values ---\n",
        "                    if val_1 == \"N/A\" or val_2 == \"N/A\":\n",
        "                        if val_1 == \"N/A\" and val_2 == \"N/A\":\n",
        "                            winner = \"tie (bothbad)\"\n",
        "                        elif val_1 == \"N/A\" and val_2 != \"N/A\":\n",
        "                            winner = \"model_b\"\n",
        "                        elif val_2 == \"N/A\" and val_1 != \"N/A\":\n",
        "                            winner = \"model_a\"\n",
        "                    else:\n",
        "                        vi, vj = float(val_1), float(val_2)\n",
        "                        if np.isclose(vi, vj, atol=1e-6):\n",
        "                            winner = \"tie\"\n",
        "                        elif (vi < vj and lower_is_better) or (vi > vj and not lower_is_better):\n",
        "                            winner = \"model_a\"\n",
        "                        else:\n",
        "                            winner = \"model_b\"\n",
        "                    info[\"winner\"] = winner\n",
        "                    results_lmsys.append(info)\n",
        "\n",
        "# After filling results_lmsys\n",
        "swapped = []\n",
        "for r in results_lmsys:\n",
        "    swapped.append({\n",
        "        \"model_a\": r[\"model_b\"],\n",
        "        \"model_b\": r[\"model_a\"],\n",
        "        \"winner\": \"model_a\" if r[\"winner\"] == \"model_b\"\n",
        "                  else \"model_b\" if r[\"winner\"] == \"model_a\"\n",
        "                  else r[\"winner\"]\n",
        "    })\n",
        "results_lmsys += swapped\n",
        "\n",
        "import pandas as pd\n",
        "battles = pd.DataFrame(results_lmsys)\n",
        "battles_no_ties = battles[~battles[\"winner\"].str.contains(\"tie\")]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e266ba9",
      "metadata": {},
      "source": [
        "## Bootstrap ELO Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "721818dc",
      "metadata": {
        "output": {
          "id": 2038023703432515,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "import pandas as pd\n",
        "\n",
        "def compute_mle_elo(\n",
        "    df, SCALE=400, BASE=10, INIT_RATING=1000, sample_weight=None\n",
        "):\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    ptbl_a_win = pd.pivot_table(\n",
        "        df[df[\"winner\"] == \"model_a\"],\n",
        "        index=\"model_a\",\n",
        "        columns=\"model_b\",\n",
        "        aggfunc=\"size\",\n",
        "        fill_value=0,\n",
        "    )\n",
        "    # if no tie, create a zero matrix\n",
        "    if sum(df[\"winner\"].isin([\"tie\", \"tie (bothbad)\"])) == 0:\n",
        "        ptbl_tie = pd.DataFrame(0, index=ptbl_a_win.index, columns=ptbl_a_win.columns)\n",
        "    else:\n",
        "        ptbl_tie = pd.pivot_table(\n",
        "            df[df[\"winner\"].isin([\"tie\", \"tie (bothbad)\"])],\n",
        "            index=\"model_a\",\n",
        "            columns=\"model_b\",\n",
        "            aggfunc=\"size\",\n",
        "            fill_value=0,\n",
        "        )\n",
        "        ptbl_tie = ptbl_tie + ptbl_tie.T\n",
        "\n",
        "    ptbl_b_win = pd.pivot_table(\n",
        "        df[df[\"winner\"] == \"model_b\"],\n",
        "        index=\"model_a\",\n",
        "        columns=\"model_b\",\n",
        "        aggfunc=\"size\",\n",
        "        fill_value=0,\n",
        "    )\n",
        "    # If you want to be explicit about aligning both rows and columns:\n",
        "    ptbl_win = (ptbl_a_win * 2).add(ptbl_b_win.T * 2, fill_value=0).add(ptbl_tie, fill_value=0)\n",
        "\n",
        "    models = pd.Series(np.arange(len(ptbl_win.index)), index=ptbl_win.index)\n",
        "\n",
        "    p = len(models)\n",
        "    X = np.zeros([p * (p - 1) * 2, p])\n",
        "    Y = np.zeros(p * (p - 1) * 2)\n",
        "\n",
        "    cur_row = 0\n",
        "    sample_weights = []\n",
        "    for m_a in ptbl_win.index:\n",
        "        for m_b in ptbl_win.columns:\n",
        "            if m_a == m_b:\n",
        "                continue\n",
        "            # if nan skip\n",
        "            if math.isnan(ptbl_win.loc[m_a, m_b]) or math.isnan(ptbl_win.loc[m_b, m_a]):\n",
        "                continue\n",
        "            X[cur_row, models[m_a]] = +math.log(BASE)\n",
        "            X[cur_row, models[m_b]] = -math.log(BASE)\n",
        "            Y[cur_row] = 1.0\n",
        "            sample_weights.append(ptbl_win.loc[m_a, m_b])\n",
        "\n",
        "            X[cur_row + 1, models[m_a]] = math.log(BASE)\n",
        "            X[cur_row + 1, models[m_b]] = -math.log(BASE)\n",
        "            Y[cur_row + 1] = 0.0\n",
        "            sample_weights.append(ptbl_win.loc[m_b, m_a])\n",
        "            cur_row += 2\n",
        "    X = X[:cur_row]\n",
        "    Y = Y[:cur_row]\n",
        "\n",
        "    lr = LogisticRegression(fit_intercept=False, penalty=None, tol=1e-6)\n",
        "    lr.fit(X, Y, sample_weight=sample_weights)\n",
        "    elo_scores = SCALE * lr.coef_[0] + INIT_RATING\n",
        "    return pd.Series(elo_scores, index=models.index).sort_values(ascending=False)\n",
        "\n",
        "def _bootstrap_worker(battles, func_compute_elo):\n",
        "    sample = battles.sample(frac=1.0, replace=True)\n",
        "    return func_compute_elo(sample)\n",
        "\n",
        "def get_bootstrap_result(battles, func_compute_elo, num_round):\n",
        "    rows = []\n",
        "    with ProcessPoolExecutor() as executor:\n",
        "        futures = [\n",
        "            executor.submit(_bootstrap_worker, battles, func_compute_elo)\n",
        "            for _ in range(num_round)\n",
        "        ]\n",
        "        for future in tqdm(as_completed(futures), total=num_round, desc=\"bootstrap\"):\n",
        "            rows.append(future.result())\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    return df[df.median().sort_values(ascending=False).index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cef69f26",
      "metadata": {
        "output": {
          "id": 1591088818862320,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "BOOTSTRAP_ROUNDS = 100\n",
        "np.random.seed(42)\n",
        "\n",
        "bootstrap_elo_lu = get_bootstrap_result(battles, compute_mle_elo, BOOTSTRAP_ROUNDS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88c6b37a",
      "metadata": {
        "output": {
          "id": 889750000503657,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "bootstrap_df = pd.DataFrame(dict(\n",
        "    lower=bootstrap_elo_lu.quantile(.025),\n",
        "    rating=bootstrap_elo_lu.quantile(.5),\n",
        "    upper=bootstrap_elo_lu.quantile(.975)\n",
        ")).reset_index(names=\"model\").sort_values(\"rating\", ascending=False)\n",
        "\n",
        "bootstrap_df[\"error_y\"] = bootstrap_df[\"upper\"] - bootstrap_df[\"rating\"]\n",
        "bootstrap_df[\"error_y_minus\"] = bootstrap_df[\"rating\"] - bootstrap_df[\"lower\"]\n",
        "bootstrap_df[\"rating_rounded\"] = np.round(bootstrap_df[\"rating\"], 2)\n",
        "bootstrap_df[[\"model\", \"rating\", \"upper\", \"lower\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b075ce2e",
      "metadata": {},
      "source": [
        "## Plot Elo Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95754b57",
      "metadata": {
        "output": {
          "id": 907053845354329,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.patches import Patch\n",
        "\n",
        "# --- 4. Create the plot (mean only) ---\n",
        "metric = \"mean\"\n",
        "display = {\"mean\": \"Mean\"}\n",
        "\n",
        "\n",
        "plt.rcParams.update({\n",
        "    \"font.family\": \"sans-serif\",\n",
        "    \"font.sans-serif\": [\"DejaVu Sans\", \"Liberation Sans\", \"Helvetica\"],\n",
        "    \"font.size\": 24,\n",
        "})\n",
        "\n",
        "\n",
        "sns.set_context(\"notebook\", font_scale=2.5)\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "\n",
        "plt.figure(figsize=(30, 8))\n",
        "\n",
        "# --- Use specified subset ---\n",
        "df = bootstrap_df[[\"model\", \"rating\", \"upper\", \"lower\"]].copy()\n",
        "\n",
        "# --- Sort methods by rating ---\n",
        "df_sorted = df.sort_values(\"rating\", ascending=False)\n",
        "\n",
        "vals = df_sorted[\"rating\"].to_numpy()             # ratings (sorted descending)\n",
        "lowers = df_sorted[\"lower\"].to_numpy()            # lower bounds\n",
        "uppers = df_sorted[\"upper\"].to_numpy()            # upper bounds\n",
        "sorted_methods = df_sorted[\"model\"].tolist()      # method names (sorted)\n",
        "\n",
        "# Calculate error bar lengths (distance from the bar value)\n",
        "lower_errors = vals - lowers\n",
        "upper_errors = uppers - vals\n",
        "\n",
        "\n",
        "errs = np.vstack([lower_errors, upper_errors])\n",
        "\n",
        "\n",
        "# --- Scaffold colors ---\n",
        "scaffold_colors = {\n",
        "    'One-Shot': \"#4A90E2\",\n",
        "    'Greedy': \"#E57373\",\n",
        "    'ReAct': \"#66BB6A\",\n",
        "    'Overall': \"#FF8C00\"\n",
        "}\n",
        "\n",
        "\n",
        "def get_scaffold(method):\n",
        "    if method.lower() == 'overall':\n",
        "        return 'Overall'\n",
        "    elif method.startswith('oneshot'):\n",
        "        return 'One-Shot'\n",
        "    elif method.startswith('greedy'):\n",
        "        return 'Greedy'\n",
        "    elif method.startswith('mlgym'):\n",
        "        return 'ReAct'\n",
        "    else:\n",
        "        return 'Overall'\n",
        "\n",
        "\n",
        "# Assign colors based on scaffold\n",
        "colors = [scaffold_colors[get_scaffold(m)] for m in sorted_methods]\n",
        "\n",
        "\n",
        "# --- x positions ---\n",
        "x = np.arange(len(sorted_methods), dtype=float)\n",
        "bar_width = 0.8  # Fatter bars\n",
        "\n",
        "\n",
        "# --- Plot bars with transparency and black outline ---\n",
        "bars = plt.bar(\n",
        "    x, vals, width=bar_width, yerr=errs, capsize=4,\n",
        "    # x, vals, width=bar_width, capsize=4,\n",
        "    color=colors, edgecolor=\"black\", alpha=0.8,\n",
        "    linewidth=0\n",
        ")\n",
        "\n",
        "\n",
        "# --- Format x-tick labels over multiple lines for long names ---\n",
        "def prettify_model_names(names):\n",
        "    mapping = {\n",
        "        \"greedy\": \"Greedy\",\n",
        "        \"sota\": \"SOTA\",\n",
        "        \"mlgym\": \"ReAct\",\n",
        "        \"oneshot\": \"One-Shot\"\n",
        "    }\n",
        "\n",
        "    prettified = []\n",
        "    for name in names:\n",
        "        parts = name.split(\"\\n\")\n",
        "        # Apply mapping to the first part only\n",
        "        first = parts[0].strip()\n",
        "        parts[0] = mapping.get(first, first)\n",
        "        prettified.append(\"\\n\".join(parts))\n",
        "\n",
        "    return prettified\n",
        "\n",
        "def format_method_name(name):\n",
        "    model_name_mapping = {\n",
        "        \"gpt-4o\": \"GPT-4o\",\n",
        "        \"cwm\": \"CWM\",\n",
        "        \"o3_mini\": \"o3-mini\",\n",
        "        \"devstral\": \"Devstral\"\n",
        "    }\n",
        "    if name.lower() == 'Overall':\n",
        "        return r'$\\mathbf{overall}$'\n",
        "    if '_' in name:\n",
        "        parts = name.split('_', 1)\n",
        "        scaffold = parts[0]\n",
        "        model = parts[1] if len(parts) > 1 else ''\n",
        "        if 'gpt-oss-20b' in model:\n",
        "            model = 'gpt-oss\\n20b'\n",
        "        elif 'gpt-oss-120b' in model:\n",
        "            model = 'gpt-oss\\n120b'\n",
        "        model = model_name_mapping.get(model, model)\n",
        "        return f\"{scaffold}\\n{model}\"\n",
        "    return name\n",
        "\n",
        "xtick_labels = [format_method_name(m) for m in sorted_methods]\n",
        "xtick_labels = prettify_model_names(xtick_labels)\n",
        "plt.xticks(x, xtick_labels, rotation=0, ha=\"center\", fontsize=25, linespacing=1.5)\n",
        "# plt.yticks(np.arange(0, 1.1, 0.1), fontsize=28)\n",
        "plt.ylabel(\"Elo\", fontsize=32, labelpad=10)\n",
        "plt.ylim(600, 1850)\n",
        "\n",
        "\n",
        "# --- Reduce space between y-axis and first/last bar ---\n",
        "plt.xlim(-0.5, len(sorted_methods) - 0.5)\n",
        "\n",
        "\n",
        "# --- Add legend for scaffold colors (excluding overall) ---\n",
        "legend_elements = [\n",
        "    Patch(facecolor=scaffold_colors['One-Shot'], label='One-Shot'),\n",
        "    Patch(facecolor=scaffold_colors['Greedy'], label='Greedy'),\n",
        "    Patch(facecolor=scaffold_colors['ReAct'], label='ReAct'),\n",
        "]\n",
        "plt.legend(handles=legend_elements, loc='upper right', fontsize=30, frameon=True,\n",
        "          title=\"Scaffold\", title_fontsize=32, edgecolor='black', fancybox=False,\n",
        "          framealpha=1.0)\n",
        "\n",
        "\n",
        "plt.grid(axis=\"y\", linestyle=\"-\", alpha=0.5, linewidth=1.2)\n",
        "plt.grid(axis=\"x\", which='both', linestyle='', linewidth=0)\n",
        "ax = plt.gca()\n",
        "for spine in ax.spines.values():\n",
        "    spine.set_visible(True)\n",
        "    spine.set_color('black')\n",
        "    spine.set_linewidth(1.5)\n",
        "\n",
        "for i in range(len(vals)):\n",
        "    plt.text(\n",
        "        x[i], vals[i] + upper_errors[i] + 0.01,  # Place label above error bar\n",
        "        f\"{vals[i]:.0f}\",      # Format to 3 decimal places\n",
        "        ha='center', va='bottom',\n",
        "        fontsize=26, fontweight='normal', color='black'\n",
        "    )\n",
        "plt.tight_layout()\n",
        "\n",
        "\n",
        "out_path = os.path.join(\"elo.pdf\")\n",
        "plt.savefig(out_path, bbox_inches=\"tight\")\n",
        "print(f\"Saved plot as: {out_path}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f71e52e",
      "metadata": {},
      "source": [
        "# Normalized Score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26be41f5",
      "metadata": {},
      "source": [
        "## Prepare Dataframe\n",
        "`full_results_rows` is a list of dicts with keys `['task_name', 'method_name', 'lower_is_better', 'sota', 'run_status', 'score', 'valid_submission', 'optimal_score']`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee729b00",
      "metadata": {
        "output": {
          "id": 889461660342345,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "full_results_rows = []\n",
        "for task_name, task_results_by_method in full_results.items():\n",
        "    for method_key, task_results in task_results_by_method.items():\n",
        "        for score, status in task_results:\n",
        "            # print(f\"Score: {score}, Status: {status}\")\n",
        "            row = {\n",
        "                \"task_name\": task_name,\n",
        "                \"method_name\": method_key,\n",
        "                \"lower_is_better\": task_metadata[task_name][\"metric_lower_is_better\"],\n",
        "                \"sota\": task_metadata[task_name][\"logging_info\"][\"sota\"][0][\"sota_score\"],\n",
        "                \"run_status\": status,\n",
        "                \"score\": score if score != \"N/A\" else None,\n",
        "                \"valid_submission\": status == \"SUCCESS\",\n",
        "                \"optimal_score\": int(not task_metadata[task_name][\"metric_lower_is_better\"]),\n",
        "            }\n",
        "            full_results_rows.append(row)\n",
        "\n",
        "experiments_df = pd.DataFrame(full_results_rows)\n",
        "experiments_df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dcc77af",
      "metadata": {},
      "source": [
        "## Calculate Normalized Scores\n",
        "Alexis' Witchcraftry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a2c9466",
      "metadata": {
        "output": {
          "id": 2682444108802847,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "def compute_worst_score(group):\n",
        "    lower_is_better = group[\"lower_is_better\"].iloc[0]\n",
        "    scores = pd.to_numeric(group[\"score\"], errors=\"coerce\")\n",
        "    return scores.max() if lower_is_better else scores.min()\n",
        "\n",
        "worst_scores = (\n",
        "    experiments_df\n",
        "    .groupby(\"task_name\")\n",
        "    .apply(compute_worst_score)\n",
        "    .reset_index(name=\"worst_score\")\n",
        ")\n",
        "\n",
        "experiments_df = experiments_df.merge(worst_scores, on=\"task_name\", how=\"left\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1926ef4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_score(df):\n",
        "    \"\"\"\n",
        "    Compute the normalized score for each row in the dataframe using existing 'worst' and 'sota' columns.\n",
        "    If 'score' is NaN, set 'normalized_score' to 0.\n",
        "    Parameters:\n",
        "    df (pandas.DataFrame): Input dataframe\n",
        "    Returns:\n",
        "    pandas.DataFrame: Updated dataframe with the new 'normalized_score' column\n",
        "    \"\"\"\n",
        "    norm = (df['score'] - df['worst_score']) / (df['sota'] - df['worst_score'])\n",
        "    df['normalized_score'] = norm.fillna(0).clip(lower=0)\n",
        "    return df\n",
        "# Apply the function to your dataframe\n",
        "experiments_df = normalize_score(experiments_df)\n",
        "\n",
        "def normalize_score_log(df):\n",
        "    diff = np.abs(df['score'] - df['optimal_score'])\n",
        "    score_log = np.where(diff == 0, -np.log10(np.abs(0.999 - df['optimal_score'])), -np.log10(np.abs(df['score'] - df['optimal_score'])))\n",
        "    worst_log = -np.log10(np.abs(df['worst_score'] - df['optimal_score']))\n",
        "    sota_log = -np.log10(np.abs(df['sota'] - df['optimal_score']))\n",
        "    norm = (score_log - worst_log) / (sota_log - worst_log)\n",
        "    df['normalized_score_log'] = pd.Series(norm).fillna(0).replace([np.inf, -np.inf], 100).clip(lower=0)\n",
        "    return df\n",
        "\n",
        "    return df\n",
        "# Apply the function to your dataframe\n",
        "experiments_df = normalize_score_log(experiments_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e396d7bd",
      "metadata": {
        "output": {
          "id": 1598574944516608,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "def parse_into_aggregate_dict(report_df, metric, algorithms=None):\n",
        "    if algorithms is None:\n",
        "        algorithms = list(report_df[\"method_name\"].unique())\n",
        "\n",
        "    # Get the unique methods\n",
        "    methods = algorithms\n",
        "    print(f\"Processing {methods}\")\n",
        "    score_dict = {}\n",
        "\n",
        "    for method in methods:\n",
        "        # Filter the dataframe for the current method\n",
        "        m_df = report_df[report_df[\"method_name\"] == method].copy()\n",
        "        # Create a 'seed' column based on the order of rows for each task\n",
        "        m_df[\"seed\"] = m_df.groupby(\"task_name\").cumcount()\n",
        "        # Pivot the dataframe: rows = seeds, columns = tasks, values = metric\n",
        "        pivot_df = m_df.pivot(index=\"seed\", columns=\"task_name\", values=metric)\n",
        "        # filter out the rows where the metric is NaN\n",
        "        # This is to ensure we only keep rows where the metric is not NaN\n",
        "        # Remove rows with NaN values\n",
        "        pivot_df = pivot_df.dropna()\n",
        "        # Convert the pivoted dataframe to a numpy matrix\n",
        "        score_dict[method] = pivot_df.to_numpy().astype(float)\n",
        "        score_dict[method] = score_dict[method][~np.isnan(score_dict[method]).any(axis=1)]\n",
        "\n",
        "    return score_dict\n",
        "\n",
        "def get_experiment_summary(\n",
        "    reports_df: pd.DataFrame,\n",
        "    experiment_ids: str | List[str],\n",
        "    *,\n",
        "    competition_ids: Optional[List[str]] = None,\n",
        "    group_label: Optional[str] = None,\n",
        "    keys: List[str] = (\"normalized_score\"),\n",
        "    all_only: bool = False,\n",
        ") -> List[dict]:\n",
        "    df = reports_df.copy()\n",
        "\n",
        "    summaries: List[dict] = []\n",
        "\n",
        "    # --------------- iterate over each experiment --------------------- #\n",
        "    for exp_id in experiment_ids:\n",
        "        exp_df = df[df[\"method_name\"] == exp_id]\n",
        "\n",
        "        # caller may restrict competitions\n",
        "        if competition_ids is not None:\n",
        "            exp_df = exp_df[exp_df[\"task_name\"].isin(competition_ids)]\n",
        "\n",
        "        if exp_df.empty:\n",
        "            print(f\"[Info] No submissions found for experiment_id '{exp_id}'.\")\n",
        "            continue\n",
        "\n",
        "        # --------------- \"all competitions\" roll-up ------------------- #\n",
        "        if exp_df[\"task_name\"].nunique() > 1:\n",
        "            all_stats = {\n",
        "                \"method_name\": exp_id,\n",
        "                \"task_name\": \"all\" if group_label is None else group_label,\n",
        "                \"total_submissions\": len(exp_df),\n",
        "            }\n",
        "            for k in keys:\n",
        "                normalized_score_dict = parse_into_aggregate_dict(exp_df, k, [exp_id])\n",
        "                aggregate_scores, aggregate_score_cis = rliable.library.get_interval_estimates(\n",
        "                    normalized_score_dict, aggregate_func, reps=50000\n",
        "                )\n",
        "                assert len(list(aggregate_scores.values())) == 1\n",
        "                assert len(list(aggregate_score_cis.values())) == 1\n",
        "                scores = list(aggregate_scores.values())[0]\n",
        "                cis = list(aggregate_score_cis.values())[0]\n",
        "                median = scores[0]\n",
        "                iqm = scores[1]\n",
        "                mean = scores[2]\n",
        "                median_lower_ci = cis[0, 0]\n",
        "                median_upper_ci = cis[1, 0]\n",
        "                iqm_lower_ci = cis[0, 1]\n",
        "                iqm_upper_ci = cis[1, 1]\n",
        "                mean_lower_ci = cis[0, 2]\n",
        "                mean_upper_ci = cis[1, 2]\n",
        "                metrics = {\n",
        "                    f\"{k}_median\": median,\n",
        "                    f\"{k}_iqm\": iqm,\n",
        "                    f\"{k}_mean\": mean,\n",
        "                    f\"{k}_median_lower_ci\": median_lower_ci,\n",
        "                    f\"{k}_median_upper_ci\": median_upper_ci,\n",
        "                    f\"{k}_iqm_lower_ci\": iqm_lower_ci,\n",
        "                    f\"{k}_iqm_upper_ci\": iqm_upper_ci,\n",
        "                    f\"{k}_mean_lower_ci\": mean_lower_ci,\n",
        "                    f\"{k}_mean_upper_ci\": mean_upper_ci,\n",
        "                }\n",
        "                all_stats.update(metrics)\n",
        "\n",
        "            summaries.append(all_stats)\n",
        "\n",
        "        if all_only:\n",
        "            continue\n",
        "        # --------------- one row per competition ---------------------- #\n",
        "        comp_list = competition_ids if competition_ids is not None else exp_df[\"task_name\"].unique()\n",
        "\n",
        "        for comp_id in comp_list:\n",
        "            comp_df = exp_df[exp_df[\"task_name\"] == comp_id]\n",
        "            if comp_df.empty:\n",
        "                continue\n",
        "\n",
        "            stats = {\n",
        "                \"experiment_id\": exp_id,\n",
        "                \"competition_id\": comp_id,\n",
        "                \"total_submissions\": len(comp_df),\n",
        "            }\n",
        "            for k in keys:\n",
        "                normalized_score_dict = parse_into_aggregate_dict(comp_df, k, [exp_id])\n",
        "                aggregate_scores, aggregate_score_cis = rliable.library.get_interval_estimates(\n",
        "                    normalized_score_dict, aggregate_func, reps=50000\n",
        "                )\n",
        "                assert len(list(aggregate_scores.values())) == 1\n",
        "                assert len(list(aggregate_score_cis.values())) == 1\n",
        "                scores = list(aggregate_scores.values())[0]\n",
        "                cis = list(aggregate_score_cis.values())[0]\n",
        "                median = scores[0]\n",
        "                iqm = scores[1]\n",
        "                mean = scores[2]\n",
        "                median_lower_ci = cis[0, 0]\n",
        "                median_upper_ci = cis[1, 0]\n",
        "                iqm_lower_ci = cis[0, 1]\n",
        "                iqm_upper_ci = cis[1, 1]\n",
        "                mean_lower_ci = cis[0, 2]\n",
        "                mean_upper_ci = cis[1, 2]\n",
        "                metrics = {\n",
        "                    f\"{k}_median\": median,\n",
        "                    f\"{k}_iqm\": iqm,\n",
        "                    f\"{k}_mean\": mean,\n",
        "                    f\"{k}_median_lower_ci\": median_lower_ci,\n",
        "                    f\"{k}_median_upper_ci\": median_upper_ci,\n",
        "                    f\"{k}_iqm_lower_ci\": iqm_lower_ci,\n",
        "                    f\"{k}_iqm_upper_ci\": iqm_upper_ci,\n",
        "                    f\"{k}_mean_lower_ci\": mean_lower_ci,\n",
        "                    f\"{k}_mean_upper_ci\": mean_upper_ci,\n",
        "                }\n",
        "                stats.update(metrics)\n",
        "            stats.update({f\"{k}_raw\": comp_df[k].tolist() for k in keys})\n",
        "            summaries.append(stats)\n",
        "\n",
        "    return summaries\n",
        "\n",
        "def aggregate_func(x):\n",
        "    return np.array(\n",
        "        [\n",
        "            rliable.metrics.aggregate_median(x),\n",
        "            rliable.metrics.aggregate_iqm(x),\n",
        "            rliable.metrics.aggregate_mean(x),\n",
        "        ]\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53604e7c",
      "metadata": {
        "output": {
          "id": 1569530330925285,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "parent_dir = os.path.dirname(os.path.abspath(os.getcwd()))\n",
        "sys.path.append(parent_dir)\n",
        "\n",
        "experiment_ids = experiments_df['method_name'].unique()\n",
        "competition_ids = experiments_df['task_name'].unique()\n",
        "\n",
        "experiment_summary = get_experiment_summary(\n",
        "    experiments_df,\n",
        "    experiment_ids=experiment_ids,\n",
        "    competition_ids=competition_ids,\n",
        "    keys=['valid_submission', 'normalized_score', 'normalized_score_log'],\n",
        "    all_only=True,\n",
        ")\n",
        "\n",
        "experiment_summary_df = pd.DataFrame(experiment_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40cedb1d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge elo columns into experiment_summary_df\n",
        "merged_df = df_sorted.merge(\n",
        "    experiment_summary_df,\n",
        "    left_on='model',\n",
        "    right_on='method_name',\n",
        "    how='left'\n",
        ")\n",
        "merged_df = merged_df.drop(columns=[\"method_name\"])\n",
        "\n",
        "# Rename ELO columns\n",
        "experiment_summary_df = merged_df.rename(\n",
        "    columns={\n",
        "        \"model\": \"method_name\",\n",
        "        'rating': 'elo_mean',\n",
        "        'upper': 'elo_upper',\n",
        "        'lower': 'elo_lower',\n",
        "\n",
        "    }\n",
        ")\n",
        "experiment_summary_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8f80423",
      "metadata": {},
      "source": [
        "### Plot Full Results\n",
        "Valid Submission Rate + Average Normalized Score + Elo Ratings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7878734",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.ticker as mticker\n",
        "\n",
        "# ---------- FIX 0: drop duplicate columns by name (keeps first copy) ----------\n",
        "experiment_summary_df = experiment_summary_df.loc[:, ~experiment_summary_df.columns.duplicated()].copy()\n",
        "\n",
        "# Define a lighter, cleaner color palette for LLMs\n",
        "llm_colors = {\n",
        "    'Devstral': '#B3F0C8',\n",
        "    'CWM': '#FFD4AD',\n",
        "    'GPT-4o': '#B3D4F9',\n",
        "    'gpt-oss-20b': '#E5DCF2',\n",
        "    'gpt-oss-120b': '#C8BAEE',\n",
        "    'o3-mini': '#FFB3B3',\n",
        "    'SOTA':  '#FFECB3'\n",
        "}\n",
        "\n",
        "# Define hatching patterns for strategies\n",
        "strategy_patterns = {\n",
        "    'One-Shot': '',\n",
        "    'Greedy': '///',\n",
        "    'ReAct': '...',\n",
        "    'SOTA': ''\n",
        "}\n",
        "\n",
        "def rename_method(method_name: str) -> str:\n",
        "    strategy_map = {\n",
        "        'oneshot': 'One-Shot',\n",
        "        'greedy': 'Greedy',\n",
        "        'mlgym': 'ReAct',\n",
        "        'sota': 'SOTA'\n",
        "    }\n",
        "    model_map = {\n",
        "        'devstral': 'Devstral',\n",
        "        'cwm': 'CWM',\n",
        "        'gpt-4o': 'GPT-4o',\n",
        "        'gpt_4o': 'GPT-4o',\n",
        "        'gpt-oss-20b': 'gpt-oss-20b',\n",
        "        'gpt_oss_20b': 'gpt-oss-20b',\n",
        "        'gpt-oss-120b': 'gpt-oss-120b',\n",
        "        'gpt_oss_120b': 'gpt-oss-120b',\n",
        "        'o3-mini': 'o3-mini',\n",
        "        'o3_mini': 'o3-mini',\n",
        "        'sota': 'SOTA'\n",
        "    }\n",
        "\n",
        "    method_name = str(method_name).strip()\n",
        "\n",
        "    if \"_\" in method_name:\n",
        "        strategy, llm = method_name.split(\"_\", 1)\n",
        "    elif \" \" in method_name:\n",
        "        strategy, llm = method_name.split(\" \", 1)\n",
        "    else:\n",
        "        strategy, llm = method_name, None\n",
        "\n",
        "    strategy = strategy_map.get(strategy, strategy)\n",
        "\n",
        "    if llm is None:\n",
        "        return strategy_map.get(method_name, model_map.get(method_name, method_name))\n",
        "\n",
        "    llm = model_map.get(llm, llm)\n",
        "    return f\"{strategy} {llm}\"\n",
        "\n",
        "# ---- Prepare data ----\n",
        "experiment_summary_df_sorted = (\n",
        "    experiment_summary_df\n",
        "    .sort_values('normalized_score_log_mean', ascending=True, na_position='last')\n",
        "    .copy()\n",
        ")\n",
        "\n",
        "experiment_summary_df_sorted['method_name'] = experiment_summary_df_sorted['method_name'].apply(rename_method)\n",
        "\n",
        "# Split renamed method into strategy + llm ONCE\n",
        "split_cols = experiment_summary_df_sorted['method_name'].str.split(' ', n=1, expand=True)\n",
        "experiment_summary_df_sorted['strategy'] = split_cols[0]\n",
        "experiment_summary_df_sorted['llm'] = split_cols[1].fillna(split_cols[0])  # \"SOTA\" -> llm=\"SOTA\"\n",
        "\n",
        "# Coerce numeric columns\n",
        "num_cols = [\n",
        "    'elo_mean', 'elo_lower', 'elo_upper',\n",
        "    'valid_submission_mean', 'valid_submission_mean_lower_ci', 'valid_submission_mean_upper_ci',\n",
        "    'normalized_score_log_mean', 'normalized_score_log_mean_lower_ci', 'normalized_score_log_mean_upper_ci'\n",
        "]\n",
        "for c in num_cols:\n",
        "    if c in experiment_summary_df_sorted.columns:\n",
        "        experiment_summary_df_sorted[c] = pd.to_numeric(experiment_summary_df_sorted[c], errors='coerce')\n",
        "\n",
        "# If duplicates exist, keep first\n",
        "experiment_summary_df_sorted = experiment_summary_df_sorted.drop_duplicates(subset=['method_name'], keep='first')\n",
        "\n",
        "methods = experiment_summary_df_sorted['method_name'].tolist()\n",
        "metrics_left = ['Valid Submission Rate', 'Average Normalized Score']\n",
        "metrics_right = ['Elo Rating']\n",
        "\n",
        "# ============================================================\n",
        "# LAYOUT KNOBS (WIDER BARS + FIT EVERYTHING)\n",
        "# ============================================================\n",
        "CLUSTER_SPAN        = 1.15   # <<< wider bar bundles per metric (increase to 1.35 for even wider)\n",
        "LEFT_METRIC_SPACING = 1.6   # more room between the two left clusters\n",
        "GAP_SCORE_TO_ELO    = 0.80   # more room between score and Elo clusters\n",
        "SCORE_AXIS_INSET    = -0.15\n",
        "X_PAD               = 0.1   # side padding so nothing clips\n",
        "# ============================================================\n",
        "\n",
        "# ---- X geometry ----\n",
        "x_left  = np.array([0.0, LEFT_METRIC_SPACING])\n",
        "x_right = np.array([x_left[1] + 1.0 + GAP_SCORE_TO_ELO])\n",
        "\n",
        "n = len(methods)\n",
        "width = CLUSTER_SPAN / n\n",
        "\n",
        "# ---- Axes: 3 y-axes total ----\n",
        "fig, ax_valid = plt.subplots(figsize=(18, 6))\n",
        "\n",
        "# Average Normalized Score axis (moved to the LEFT of the avg-score bars)\n",
        "ax_score = ax_valid.twinx()\n",
        "ax_score.patch.set_visible(False)\n",
        "ax_score.tick_params(axis='x', bottom=False, labelbottom=False)\n",
        "\n",
        "# Elo axis on the far right\n",
        "ax_elo = ax_valid.twinx()\n",
        "ax_elo.patch.set_visible(False)\n",
        "ax_elo.tick_params(axis='x', bottom=False, labelbottom=False)\n",
        "\n",
        "# Place the avg-score y-axis just LEFT of the avg-score bar cluster\n",
        "score_axis_x = x_left[1] - 0.5 + SCORE_AXIS_INSET\n",
        "ax_score.spines[\"left\"].set_position((\"data\", score_axis_x))\n",
        "ax_score.spines[\"left\"].set_visible(True)\n",
        "ax_score.spines[\"right\"].set_visible(False)\n",
        "ax_score.spines[\"top\"].set_visible(False)\n",
        "ax_score.spines[\"bottom\"].set_visible(False)\n",
        "ax_score.yaxis.set_label_position(\"left\")\n",
        "ax_score.yaxis.tick_left()\n",
        "\n",
        "# ---- Plot ----\n",
        "for i, (_, row) in enumerate(experiment_summary_df_sorted.iterrows()):\n",
        "    method = row['method_name']\n",
        "    strategy = row['strategy']\n",
        "    llm = row['llm']\n",
        "\n",
        "    color = llm_colors.get(llm, '#000000')\n",
        "    hatch = strategy_patterns.get(strategy, '')\n",
        "\n",
        "    # Centered offsets for bars within each metric cluster\n",
        "    offset = (i - (n - 1) / 2) * width\n",
        "\n",
        "    # 1) Valid Submission Rate (LEFT axis, 0100) - skip SOTA\n",
        "    if method != \"SOTA\" and np.isfinite(row['valid_submission_mean']):\n",
        "        value = row['valid_submission_mean'] * 100\n",
        "        err_low = (row['valid_submission_mean'] - row['valid_submission_mean_lower_ci']) * 100\n",
        "        err_up  = (row['valid_submission_mean_upper_ci'] - row['valid_submission_mean']) * 100\n",
        "\n",
        "        bar = ax_valid.bar(\n",
        "            x_left[0] + offset, value, width,\n",
        "            yerr=[[err_low], [err_up]],\n",
        "            label=method, capsize=3, alpha=0.8,\n",
        "            color=color, hatch=hatch, edgecolor='black', linewidth=0.5\n",
        "        )[0]\n",
        "        ax_valid.text(\n",
        "            bar.get_x() + bar.get_width()/2., value + err_up + 1,\n",
        "            f'{value:.0f}', ha='center', va='bottom', fontsize=10\n",
        "        )\n",
        "\n",
        "    # 2) Average Normalized Score (DECIMAL 01) - skip SOTA\n",
        "    if method != \"SOTA\" and np.isfinite(row['normalized_score_log_mean']):\n",
        "        value = row['normalized_score_log_mean']  # decimal\n",
        "        err_low = row['normalized_score_log_mean'] - row['normalized_score_log_mean_lower_ci']\n",
        "        err_up  = row['normalized_score_log_mean_upper_ci'] - row['normalized_score_log_mean']\n",
        "\n",
        "        bar = ax_score.bar(\n",
        "            x_left[1] + offset, value, width,\n",
        "            yerr=[[err_low], [err_up]],\n",
        "            label=None, capsize=3, alpha=0.8,\n",
        "            color=color, hatch=hatch, edgecolor='black', linewidth=0.5\n",
        "        )[0]\n",
        "        ax_score.text(\n",
        "            bar.get_x() + bar.get_width()/2., value + err_up + 0.01,\n",
        "            f'{value:.2f}', ha='center', va='bottom', fontsize=8\n",
        "        )\n",
        "\n",
        "    # 3) Elo Rating (RIGHT axis) - include SOTA\n",
        "    if np.isfinite(row['elo_mean']) and np.isfinite(row['elo_lower']) and np.isfinite(row['elo_upper']):\n",
        "        value = row['elo_mean']\n",
        "        err_low = row['elo_mean'] - row['elo_lower']\n",
        "        err_up  = row['elo_upper'] - row['elo_mean']\n",
        "\n",
        "        label_for_legend = method if method == \"SOTA\" else None\n",
        "\n",
        "        bar = ax_elo.bar(\n",
        "            x_right[0] + offset, value, width,\n",
        "            yerr=[[err_low], [err_up]],\n",
        "            label=label_for_legend, capsize=3, alpha=0.8,\n",
        "            color=color, hatch=hatch, edgecolor='black', linewidth=0.5\n",
        "        )[0]\n",
        "\n",
        "        vertical_offset = 15 if i % 2 == 0 else 5\n",
        "        ax_elo.text(\n",
        "            bar.get_x() + bar.get_width()/2., value + err_up + vertical_offset,\n",
        "            f'{value:.0f}', ha='center', va='bottom', fontsize=8\n",
        "        )\n",
        "\n",
        "# Separator between AvgScore and Elo\n",
        "sep_x = (x_left[1] + x_right[0]) / 2.0\n",
        "ax_valid.axvline(x=sep_x, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
        "\n",
        "# ---- Labels/limits ----\n",
        "ax_valid.set_ylabel('Valid submission rate', fontsize=15, fontweight='bold')\n",
        "ax_score.set_ylabel('Average normalized score', fontsize=15, fontweight='bold')\n",
        "ax_elo.set_ylabel('Elo Rating', fontsize=15, fontweight='bold')\n",
        "\n",
        "ax_valid.set_ylim(0, 105)\n",
        "ax_score.set_ylim(0, 1.05)\n",
        "ax_elo.set_ylim(bottom=650)\n",
        "\n",
        "ax_score.yaxis.set_major_locator(mticker.FixedLocator(np.arange(0.2, 1.01, 0.2)))\n",
        "ax_score.yaxis.set_major_formatter(mticker.FormatStrFormatter('%.2f'))\n",
        "\n",
        "ax_valid.tick_params(axis='y', labelsize=13)\n",
        "ax_score.tick_params(axis='y', labelsize=13)\n",
        "ax_elo.tick_params(axis='y', labelsize=13)\n",
        "\n",
        "# X ticks\n",
        "all_x = np.concatenate([x_left, x_right])\n",
        "ax_valid.set_xticks(all_x)\n",
        "ax_valid.set_xticklabels(metrics_left + metrics_right, fontsize=14)\n",
        "ax_valid.get_xaxis().set_visible(False)\n",
        "\n",
        "# X-limits (auto-fit to chosen CLUSTER_SPAN)\n",
        "cluster_half = CLUSTER_SPAN / 2\n",
        "ax_valid.set_xlim(x_left[0] - cluster_half - X_PAD,\n",
        "                  x_right[0] + cluster_half + X_PAD)\n",
        "\n",
        "# Legend: collect from valid axis + elo axis so SOTA appears (score axis has no labels)\n",
        "h1, l1 = ax_valid.get_legend_handles_labels()\n",
        "h3, l3 = ax_elo.get_legend_handles_labels()\n",
        "\n",
        "leg = ax_valid.legend(\n",
        "    h1 + h3, l1 + l3,\n",
        "    loc='upper center',\n",
        "    bbox_to_anchor=(0.68, 1.0),\n",
        "    title='Agents',\n",
        "    framealpha=0.8,\n",
        "    fontsize=10,\n",
        ")\n",
        "leg.get_title().set_fontsize(12)\n",
        "leg.get_title().set_fontweight('bold')\n",
        "\n",
        "# Grid\n",
        "ax_valid.grid(axis='y', alpha=0.3)\n",
        "ax_score.grid(False)\n",
        "ax_elo.grid(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('fig_metrics_summary.pdf', format='pdf', bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Valid Submission Rate (VSR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary Plots\n",
        "\n",
        "scaffold_colors = {'oneshot': \"#4A90E2\", 'greedy': \"#E57373\", 'mlgym': \"#66BB6A\", 'overall': \"#FF8C00\"}\n",
        "\n",
        "def get_scaffold(method):\n",
        "    if method.lower() == 'overall': return 'overall'\n",
        "    elif method.startswith('oneshot'): return 'oneshot'\n",
        "    elif method.startswith('greedy'): return 'greedy'\n",
        "    elif method.startswith('mlgym'): return 'mlgym'\n",
        "    return 'overall'\n",
        "\n",
        "def format_method_name(name):\n",
        "    if name.lower() == 'overall': return r'$\\mathbf{Overall}$'\n",
        "    if '_' in name:\n",
        "        parts = name.split('_', 1)\n",
        "        scaffold = {'mlgym': 'ReAct', 'oneshot': 'One-Shot', 'greedy': 'Greedy'}.get(parts[0], parts[0])\n",
        "        model = parts[1] if len(parts) > 1 else ''\n",
        "        model = model.replace('gpt-oss-20b', 'gpt-oss\\n20b').replace('gpt-oss-120b', 'gpt-oss\\n120b')\n",
        "        model = model.replace('gpt-4o', 'GPT-4o').replace('o3_mini', 'o3-mini')\n",
        "        model = model.replace('cwm', 'CWM').replace('devstral', 'Devstral')\n",
        "        return f\"{scaffold}\\n{model}\"\n",
        "    return name\n",
        "\n",
        "def mean_ci(series):\n",
        "    n = len(series)\n",
        "    mean = series.mean()\n",
        "    sem = series.sem() if n > 1 else 0\n",
        "    h = sem * t.ppf(0.975, n-1) if n > 1 else 0\n",
        "    return mean, mean-h, mean+h\n",
        "\n",
        "plt.rcParams.update({\"font.family\": \"sans-serif\", \"font.sans-serif\": [\"DejaVu Sans\"], \"font.size\": 24})\n",
        "sns.set_context(\"notebook\", font_scale=2.5)\n",
        "sns.set_style(\"whitegrid\")\n",
        "print(\"Setup complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- VSR Bar Chart with Error Bars ---\n",
        "vsr_records = []\n",
        "for task_name, agents_dict in full_results.items():\n",
        "    for agent, runs in agents_dict.items():\n",
        "        for seed_idx, (score, status) in enumerate(runs):\n",
        "            vsr_records.append({'task': task_name, 'method': agent, 'is_valid': (status == 'SUCCESS'), 'seed': seed_idx})\n",
        "\n",
        "df_vsr = pd.DataFrame(vsr_records)\n",
        "vsr_per_task = df_vsr.groupby(['method', 'task'])['is_valid'].mean().reset_index()\n",
        "vsr_per_task.columns = ['method', 'task', 'vsr']\n",
        "\n",
        "method_vsr_stats = {}\n",
        "for method in vsr_per_task['method'].unique():\n",
        "    vals = vsr_per_task[vsr_per_task['method'] == method]['vsr']\n",
        "    mean_val, lower, upper = mean_ci(vals)\n",
        "    method_vsr_stats[method] = {'mean': mean_val, 'lower': lower, 'upper': upper}\n",
        "\n",
        "all_vsr = vsr_per_task['vsr']\n",
        "overall_mean, overall_lower, overall_upper = mean_ci(all_vsr)\n",
        "method_vsr_stats['Overall'] = {'mean': overall_mean, 'lower': overall_lower, 'upper': overall_upper}\n",
        "\n",
        "methods_sorted = sorted(method_vsr_stats.keys(), key=lambda m: method_vsr_stats[m]['mean'], reverse=True)\n",
        "means = np.array([method_vsr_stats[m]['mean'] for m in methods_sorted])\n",
        "lowers = np.clip([method_vsr_stats[m]['lower'] for m in methods_sorted], 0, 1)\n",
        "uppers = np.clip([method_vsr_stats[m]['upper'] for m in methods_sorted], 0, 1)\n",
        "errs = np.vstack([np.clip(means - lowers, 0, means), np.clip(uppers - means, 0, 1 - means)])\n",
        "\n",
        "plt.figure(figsize=(30, 8))\n",
        "x = np.arange(len(methods_sorted))\n",
        "bars = plt.bar(x, means, width=0.8, yerr=errs, capsize=4, color=[scaffold_colors[get_scaffold(m)] for m in methods_sorted], edgecolor=\"black\", alpha=0.8, linewidth=0)\n",
        "for i, mean in enumerate(means):\n",
        "    plt.text(x[i], mean + errs[1,i] + 0.01, f\"{mean:.2f}\", ha='center', va='bottom', fontsize=26)\n",
        "plt.xticks(x, [format_method_name(m) for m in methods_sorted], fontsize=25, linespacing=1.5)\n",
        "plt.yticks(np.arange(0, 1.10, 0.1), fontsize=28)\n",
        "plt.ylabel(\"Valid Submission Rate\", fontsize=32)\n",
        "plt.ylim(0, 1.10)\n",
        "plt.xlim(-0.5, len(methods_sorted) - 0.5)\n",
        "legend_elements = [Patch(facecolor=scaffold_colors['oneshot'], label='One-Shot'), Patch(facecolor=scaffold_colors['greedy'], label='Greedy'), Patch(facecolor=scaffold_colors['mlgym'], label='ReAct')]\n",
        "plt.legend(handles=legend_elements, loc='upper right', fontsize=30, title=\"Scaffold\", title_fontsize=32)\n",
        "plt.grid(axis=\"y\", linestyle=\"-\", alpha=0.5)\n",
        "for spine in plt.gca().spines.values(): spine.set_visible(True); spine.set_color('black'); spine.set_linewidth(1.5)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"fig_valid_submission_rate.pdf\", bbox_inches=\"tight\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. VSR - Stacked Bar Chart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- VSR Stacked Bar Chart (matching source notebook style) ---\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\", font_scale=2.0, rc={\n",
        "    \"axes.edgecolor\": \"#333333\",\n",
        "    \"axes.linewidth\": 1.5,\n",
        "})\n",
        "\n",
        "categories = [\n",
        "    \"Invalid\",\n",
        "    \"Low (1-33%)\",\n",
        "    \"Medium (34-66%)\",\n",
        "    \"High (67-100%)\"\n",
        "]\n",
        "colors = [\n",
        "    \"#d0d0d0\",\n",
        "    \"#ff8b6d\",\n",
        "    \"#ffdb9a\",\n",
        "    \"#95d5b2\",\n",
        "]\n",
        "\n",
        "# Compute per-method valid submission rates per task\n",
        "method_distributions = {}\n",
        "methods = df_vsr['method'].unique()\n",
        "for method in methods:\n",
        "    df_m = df_vsr[df_vsr['method'] == method]\n",
        "    # Group by task, compute valid submission rate per task\n",
        "    rates = df_m.groupby('task')['is_valid'].mean() * 100  # percent\n",
        "    # Bin into categories\n",
        "    counts = [\n",
        "        np.sum(rates == 0),\n",
        "        np.sum((rates > 0) & (rates <= 33)),\n",
        "        np.sum((rates > 33) & (rates <= 66)),\n",
        "        np.sum(rates > 66),\n",
        "    ]\n",
        "    total = sum(counts)\n",
        "    percentages = [c/total*100 if total > 0 else 0 for c in counts]\n",
        "    method_distributions[method] = (counts, percentages)\n",
        "\n",
        "# Sort methods by \"Medium\" + \"High\" percentage (descending)\n",
        "medium_high_percentages = [(method, method_distributions[method][1][2] + method_distributions[method][1][3]) for method in methods]\n",
        "medium_high_percentages.sort(key=lambda x: x[1], reverse=True)\n",
        "sorted_methods = [m for m, _ in medium_high_percentages]\n",
        "\n",
        "methods_sorted_formatted = [format_method_name(m) for m in sorted_methods]\n",
        "counts_array = np.array([method_distributions[m][0] for m in sorted_methods])\n",
        "percentages_array = np.array([method_distributions[m][1] for m in sorted_methods])\n",
        "x = np.arange(len(sorted_methods))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(72, 18))\n",
        "bottom = np.zeros(len(sorted_methods))\n",
        "bar_width = 0.5\n",
        "\n",
        "for i, label in enumerate(categories):\n",
        "    ax.bar(\n",
        "        x,\n",
        "        percentages_array[:, i],\n",
        "        width=bar_width,\n",
        "        bottom=bottom,\n",
        "        label=label,\n",
        "        color=colors[i],\n",
        "        alpha=0.85,\n",
        "        linewidth=0.0,\n",
        "        edgecolor='white'\n",
        "    )\n",
        "    bottom += percentages_array[:, i]\n",
        "\n",
        "ax.set_ylabel(\"Task Percentage (%)\", labelpad=25, fontsize=64)\n",
        "ax.tick_params(axis='y', labelsize=60, width=2, length=10)\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(methods_sorted_formatted, rotation=0, ha=\"center\", fontsize=54, linespacing=1.5)\n",
        "ax.tick_params(axis='x', labelsize=54, width=2, length=8, pad=10)\n",
        "ax.grid(axis=\"y\", linestyle=\"-\", alpha=0.3, linewidth=1.5, zorder=0)\n",
        "ax.xaxis.grid(False)\n",
        "ax.set_axisbelow(True)\n",
        "ax.set_ylim(0, 108)\n",
        "ax.set_yticks(np.arange(0.0, 100.1, 12.5))\n",
        "\n",
        "# Add percentage labels inside bars\n",
        "min_height_for_text = 8\n",
        "for i, method in enumerate(sorted_methods):\n",
        "    cumulative = 0\n",
        "    for j in range(4):\n",
        "        pct = percentages_array[i, j]\n",
        "        if pct > 0 and pct >= min_height_for_text:\n",
        "            fontsize = 60\n",
        "            text_color = '#2C3E50'\n",
        "            ax.text(\n",
        "                i, cumulative + pct / 2,\n",
        "                f'{pct:.1f}',\n",
        "                ha='center', va='center',\n",
        "                fontsize=fontsize,\n",
        "                color=text_color,\n",
        "                fontweight='normal'\n",
        "            )\n",
        "        cumulative += pct\n",
        "\n",
        "legend = ax.legend(\n",
        "    title=\"Submission Rate\",\n",
        "    frameon=True,\n",
        "    loc='center left',\n",
        "    bbox_to_anchor=(1.01, 0.5),\n",
        "    ncol=1,\n",
        "    framealpha=1.0,\n",
        "    edgecolor='#333333',\n",
        "    fancybox=False,\n",
        "    shadow=False,\n",
        "    fontsize=54,\n",
        "    title_fontsize=58\n",
        ")\n",
        "legend.get_frame().set_linewidth(2)\n",
        "\n",
        "for spine in ax.spines.values():\n",
        "    spine.set_linewidth(3)\n",
        "    spine.set_edgecolor('#222222')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"fig_vsr_stacked.pdf\", bbox_inches=\"tight\", dpi=300, facecolor='white')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Average Normalized Score (ANS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- ANS Bar Chart with Error Bars (using experiment_summary_df) ---\n",
        "\n",
        "# Prepare data from experiment_summary_df\n",
        "df_plot = experiment_summary_df.copy()\n",
        "df_plot = df_plot[df_plot['method_name'] != 'Overall']  # Exclude if exists\n",
        "\n",
        "# Rename for consistency\n",
        "df_plot['method'] = df_plot['method_name']\n",
        "\n",
        "# Use normalized_score_log_mean as the ANS metric\n",
        "method_ans_stats = {}\n",
        "for _, row in df_plot.iterrows():\n",
        "    method = row['method']\n",
        "    if pd.notna(row['normalized_score_log_mean']):\n",
        "        method_ans_stats[method] = {\n",
        "            'mean': row['normalized_score_log_mean'],\n",
        "            'lower': row['normalized_score_log_mean_lower_ci'],\n",
        "            'upper': row['normalized_score_log_mean_upper_ci']\n",
        "        }\n",
        "\n",
        "# Add overall (average across all methods)\n",
        "all_means = df_plot['normalized_score_log_mean'].dropna()\n",
        "if len(all_means) > 0:\n",
        "    overall_mean = all_means.mean()\n",
        "    overall_std = all_means.std()\n",
        "    n = len(all_means)\n",
        "    overall_ci = 1.96 * overall_std / np.sqrt(n) if n > 1 else 0\n",
        "    method_ans_stats['Overall'] = {\n",
        "        'mean': overall_mean,\n",
        "        'lower': overall_mean - overall_ci,\n",
        "        'upper': overall_mean + overall_ci\n",
        "    }\n",
        "\n",
        "# Sort methods by ANS (descending)\n",
        "methods_sorted_ans = sorted(method_ans_stats.keys(), key=lambda m: method_ans_stats[m]['mean'], reverse=True)\n",
        "means = np.array([method_ans_stats[m]['mean'] for m in methods_sorted_ans])\n",
        "lowers = np.clip([method_ans_stats[m]['lower'] for m in methods_sorted_ans], 0, 1)\n",
        "uppers = np.clip([method_ans_stats[m]['upper'] for m in methods_sorted_ans], 0, 1)\n",
        "errs = np.vstack([np.clip(means - lowers, 0, means), np.clip(uppers - means, 0, 1 - means)])\n",
        "\n",
        "plt.figure(figsize=(30, 8))\n",
        "x = np.arange(len(methods_sorted_ans))\n",
        "bars = plt.bar(x, means, width=0.8, yerr=errs, capsize=4,\n",
        "               color=[scaffold_colors[get_scaffold(m)] for m in methods_sorted_ans],\n",
        "               edgecolor=\"black\", alpha=0.8, linewidth=0)\n",
        "\n",
        "for i, mean in enumerate(means):\n",
        "    plt.text(x[i], mean + errs[1,i] + 0.01, f\"{mean:.3f}\", ha='center', va='bottom', fontsize=26)\n",
        "\n",
        "plt.xticks(x, [format_method_name(m) for m in methods_sorted_ans], fontsize=25, linespacing=1.5)\n",
        "plt.yticks(np.arange(0, 0.75, 0.05), fontsize=28)\n",
        "plt.ylabel(\"Normalized Score\", fontsize=32)\n",
        "plt.ylim(0, 0.60)\n",
        "plt.xlim(-0.5, len(methods_sorted_ans) - 0.5)\n",
        "\n",
        "legend_elements = [\n",
        "    Patch(facecolor=scaffold_colors['oneshot'], label='One-Shot'),\n",
        "    Patch(facecolor=scaffold_colors['greedy'], label='Greedy'),\n",
        "    Patch(facecolor=scaffold_colors['mlgym'], label='ReAct')\n",
        "]\n",
        "plt.legend(handles=legend_elements, loc='upper right', fontsize=30, title=\"Scaffold\", title_fontsize=32)\n",
        "plt.grid(axis=\"y\", linestyle=\"-\", alpha=0.5)\n",
        "\n",
        "for spine in plt.gca().spines.values():\n",
        "    spine.set_visible(True)\n",
        "    spine.set_color('black')\n",
        "    spine.set_linewidth(1.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"fig_avg_normalized_score.pdf\", bbox_inches=\"tight\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. ANS - Stacked Bar Chart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- ANS Stacked Bar Chart (Performance Categories - Task Specific) ---\n",
        "# MATCHING SOURCE NOTEBOOK LOGIC EXACTLY\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\", font_scale=2.0, rc={\n",
        "    \"axes.edgecolor\": \"#333333\",\n",
        "    \"axes.linewidth\": 1.5,\n",
        "})\n",
        "\n",
        "colors = [\"#d0d0d0\", \"#ff8b6d\", \"#ffdb9a\", \"#6ec6ff\", \"#95d5b2\"]\n",
        "bins = [\"Invalid\", \"Worst\", \"Below Avg\", \"Above Avg\", \"Best\"]\n",
        "\n",
        "# ==================== STEP 1: Build raw records ====================\n",
        "records = []\n",
        "for task_name, agents_dict in full_results.items():\n",
        "    if task_name not in task_metadata:\n",
        "        continue\n",
        "    meta = task_metadata[task_name]\n",
        "    logging_info = meta.get('logging_info', {})\n",
        "    sota_list = logging_info.get('sota', [])\n",
        "    sota = sota_list[0].get('sota_score') if sota_list else None\n",
        "    lower_is_better = meta.get('metric_lower_is_better', False)\n",
        "\n",
        "    if sota is None:\n",
        "        continue\n",
        "\n",
        "    for agent, runs in agents_dict.items():\n",
        "        for seed_idx, (score, status) in enumerate(runs):\n",
        "            is_valid = (status == 'SUCCESS')\n",
        "            try:\n",
        "                score_val = float(score) if is_valid else None\n",
        "            except:\n",
        "                score_val = None\n",
        "\n",
        "            records.append({\n",
        "                'task': task_name,\n",
        "                'method': agent,\n",
        "                'score': score_val,\n",
        "                'lower_is_better': lower_is_better,\n",
        "                'sota': float(sota),\n",
        "                'is_valid': is_valid,\n",
        "                'seed': seed_idx\n",
        "            })\n",
        "\n",
        "df = pd.DataFrame(records)\n",
        "print(f\"Built df with {len(df)} records\")\n",
        "\n",
        "# ==================== STEP 2: Compute worst score per task from ACTUAL valid scores ====================\n",
        "worst_score_per_task = {}\n",
        "for task in df['task'].unique():\n",
        "    task_data = df[(df['task'] == task) & (df['is_valid'] == True)]\n",
        "    if len(task_data) > 0:\n",
        "        lower_is_better = task_data['lower_is_better'].iloc[0]\n",
        "        if lower_is_better:\n",
        "            worst_score_per_task[task] = task_data['score'].max()\n",
        "        else:\n",
        "            worst_score_per_task[task] = task_data['score'].min()\n",
        "    else:\n",
        "        worst_score_per_task[task] = None\n",
        "\n",
        "df['worst_score'] = df['task'].map(worst_score_per_task)\n",
        "\n",
        "# ==================== STEP 3: March of 9's transform ====================\n",
        "def march_of_9s_normalized(row):\n",
        "    if not row['is_valid'] or row['score'] is None or pd.isna(row['score']):\n",
        "        return 0.0\n",
        "\n",
        "    score = row['score']\n",
        "    sota = row['sota']\n",
        "    worst = row['worst_score']\n",
        "    lower_is_better = row['lower_is_better']\n",
        "\n",
        "    optimal = 0.0 if lower_is_better else 1.0\n",
        "    diff = abs(score - optimal)\n",
        "    if diff == 0:\n",
        "        diff = abs(0.999 - optimal)\n",
        "\n",
        "    march = -np.log10(diff)\n",
        "    sota_diff = abs(sota - optimal)\n",
        "    worst_diff = abs(worst - optimal)\n",
        "    sota_march = -np.log10(sota_diff)\n",
        "    worst_march = -np.log10(worst_diff)\n",
        "\n",
        "    if np.isclose(sota_march, worst_march):\n",
        "        return np.nan\n",
        "\n",
        "    normalized = (march - worst_march) / (sota_march - worst_march)\n",
        "    return max(0, normalized)\n",
        "\n",
        "df['normalized_march'] = df.apply(march_of_9s_normalized, axis=1)\n",
        "\n",
        "# ==================== STEP 4: Average per method per task ====================\n",
        "df_avg = df[df['method'] != 'Overall'].groupby(['task', 'method'], as_index=False).agg({\n",
        "    'normalized_march': 'mean',\n",
        "    'lower_is_better': 'first',\n",
        "    'sota': 'first',\n",
        "}).dropna(subset=['normalized_march'])\n",
        "\n",
        "print(f\"Built df_avg with {len(df_avg)} records\")\n",
        "\n",
        "# ==================== STEP 5: Task-specific binning (from source) ====================\n",
        "method_task_avg = df_avg.groupby(['task', 'method'], as_index=False)['normalized_march'].mean()\n",
        "methods = method_task_avg['method'].unique()\n",
        "common_tasks = method_task_avg['task'].unique()\n",
        "\n",
        "def categorize_method_for_task(task_data, method_score):\n",
        "    if pd.isna(method_score) or method_score == 0:\n",
        "        return 0\n",
        "    valid_scores = task_data['normalized_march'].dropna()\n",
        "    valid_scores = valid_scores[valid_scores != 0]\n",
        "    if len(valid_scores) == 0:\n",
        "        return 0\n",
        "    task_avg = valid_scores.mean()\n",
        "    task_best = valid_scores.max()\n",
        "    task_worst = valid_scores.min()\n",
        "    if task_best == task_worst:\n",
        "        return 3\n",
        "    if abs(method_score - task_best) < 1e-10:\n",
        "        return 4\n",
        "    elif abs(method_score - task_worst) < 1e-10:\n",
        "        return 1\n",
        "    elif method_score > task_avg:\n",
        "        return 3\n",
        "    else:\n",
        "        return 2\n",
        "\n",
        "method_categories = {}\n",
        "for method in methods:\n",
        "    method_data = method_task_avg[method_task_avg['method'] == method]\n",
        "    category_counts = [0, 0, 0, 0, 0]\n",
        "    for task in common_tasks:\n",
        "        task_data = method_task_avg[method_task_avg['task'] == task]\n",
        "        method_score_data = method_data[method_data['task'] == task]['normalized_march']\n",
        "        method_score = method_score_data.iloc[0] if len(method_score_data) > 0 else np.nan\n",
        "        category = categorize_method_for_task(task_data, method_score)\n",
        "        category_counts[category] += 1\n",
        "    method_categories[method] = category_counts\n",
        "\n",
        "# ==================== STEP 6: Sorting and plotting ====================\n",
        "total_tasks = len(common_tasks)\n",
        "method_scores = {m: (np.array(method_categories[m][:5]) / total_tasks * 100)[3] + (np.array(method_categories[m][:5]) / total_tasks * 100)[4] for m in methods}\n",
        "methods_sorted = sorted(method_scores.keys(), key=lambda x: method_scores[x], reverse=True)\n",
        "methods_sorted_formatted = [format_method_name(m) for m in methods_sorted]\n",
        "\n",
        "counts_array = np.array([method_categories[m][:5] for m in methods_sorted])\n",
        "percentage_array = (counts_array / total_tasks) * 100\n",
        "x = np.arange(len(methods_sorted))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(72, 18))\n",
        "bottom = np.zeros(len(methods_sorted))\n",
        "bar_width = 0.5\n",
        "\n",
        "for i, label in enumerate(bins):\n",
        "    ax.bar(x, percentage_array[:, i], width=bar_width, bottom=bottom,\n",
        "           label=label, color=colors[i], alpha=0.85, linewidth=0.0, edgecolor='white')\n",
        "    bottom += percentage_array[:, i]\n",
        "\n",
        "ax.set_ylabel(\"Task Percentage (%)\", labelpad=25, fontsize=64)\n",
        "ax.tick_params(axis='y', labelsize=60, width=2, length=10)\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(methods_sorted_formatted, rotation=0, ha=\"center\", fontsize=54, linespacing=1.5)\n",
        "ax.tick_params(axis='x', labelsize=54, width=2, length=8, pad=10)\n",
        "ax.grid(axis=\"y\", linestyle=\"-\", alpha=0.3, linewidth=1.5, zorder=0)\n",
        "ax.xaxis.grid(False)\n",
        "ax.set_axisbelow(True)\n",
        "ax.set_ylim(0, 108)\n",
        "ax.set_yticks(np.arange(0, 101, 12.5))\n",
        "\n",
        "min_height_for_text = 8\n",
        "for i, method in enumerate(methods_sorted):\n",
        "    cumulative = 0\n",
        "    for j in range(5):\n",
        "        pct = percentage_array[i, j]\n",
        "        if pct > 0 and pct >= min_height_for_text:\n",
        "            ax.text(i, cumulative + pct / 2, f\"{pct:.1f}\",\n",
        "                    ha='center', va='center', fontsize=60, color='#2C3E50', fontweight='normal')\n",
        "        cumulative += pct\n",
        "\n",
        "legend = ax.legend(title=\"Performance Category\", frameon=True, loc='center left',\n",
        "                   bbox_to_anchor=(1.01, 0.5), ncol=1, framealpha=1.0,\n",
        "                   edgecolor='#333333', fancybox=False, shadow=False,\n",
        "                   fontsize=54, title_fontsize=58)\n",
        "legend.get_frame().set_linewidth(2)\n",
        "\n",
        "for spine in ax.spines.values():\n",
        "    spine.set_linewidth(3)\n",
        "    spine.set_edgecolor('#222222')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"fig_performance_categories_stackedbar_taskspecific.pdf\", bbox_inches=\"tight\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. ANS Per Task (March of 9's) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- March of 9's Scatter Plot (Per Task) ---\n",
        "\n",
        "# ==================== BUILD DATA FROM full_results ====================\n",
        "records = []\n",
        "for task_name, agents_dict in full_results.items():\n",
        "    if task_name not in task_metadata:\n",
        "        continue\n",
        "    meta = task_metadata[task_name]\n",
        "    logging_info = meta.get('logging_info', {})\n",
        "    sota_list = logging_info.get('sota', [])\n",
        "    sota = sota_list[0].get('sota_score') if sota_list else None\n",
        "    lower_is_better = meta.get('metric_lower_is_better', False)\n",
        "\n",
        "    if sota is None:\n",
        "        continue\n",
        "\n",
        "    for agent, runs in agents_dict.items():\n",
        "        for seed_idx, (score, status) in enumerate(runs):\n",
        "            is_valid = (status == 'SUCCESS')\n",
        "            try:\n",
        "                score_val = float(score) if is_valid else None\n",
        "            except:\n",
        "                score_val = None\n",
        "            records.append({\n",
        "                'task': task_name,\n",
        "                'method': agent,\n",
        "                'score': score_val,\n",
        "                'lower_is_better': lower_is_better,\n",
        "                'sota': float(sota),\n",
        "                'is_valid': is_valid,\n",
        "                'seed': seed_idx\n",
        "            })\n",
        "\n",
        "df = pd.DataFrame(records)\n",
        "common_tasks = df['task'].unique()\n",
        "\n",
        "# Compute worst score per task from actual valid scores\n",
        "worst_score_per_task = {}\n",
        "for task in df['task'].unique():\n",
        "    task_data = df[(df['task'] == task) & (df['is_valid'] == True)]\n",
        "    if len(task_data) > 0:\n",
        "        lower_is_better = task_data['lower_is_better'].iloc[0]\n",
        "        worst_score_per_task[task] = task_data['score'].max() if lower_is_better else task_data['score'].min()\n",
        "    else:\n",
        "        worst_score_per_task[task] = None\n",
        "df['worst_score'] = df['task'].map(worst_score_per_task)\n",
        "\n",
        "# March of 9's transform\n",
        "def march_of_9s_normalized(row):\n",
        "    if not row['is_valid'] or row['score'] is None or pd.isna(row['score']):\n",
        "        return 0.0\n",
        "    score, sota, worst = row['score'], row['sota'], row['worst_score']\n",
        "    lower_is_better = row['lower_is_better']\n",
        "    optimal = 0.0 if lower_is_better else 1.0\n",
        "    diff = abs(score - optimal)\n",
        "    if diff == 0:\n",
        "        diff = abs(0.999 - optimal)\n",
        "    march = -np.log10(diff)\n",
        "    sota_diff, worst_diff = abs(sota - optimal), abs(worst - optimal)\n",
        "    sota_march, worst_march = -np.log10(sota_diff), -np.log10(worst_diff)\n",
        "    if np.isclose(sota_march, worst_march):\n",
        "        return np.nan\n",
        "    normalized = (march - worst_march) / (sota_march - worst_march)\n",
        "    return max(0, normalized)\n",
        "\n",
        "df['normalized_march'] = df.apply(march_of_9s_normalized, axis=1)\n",
        "\n",
        "# Average per method per task\n",
        "df_avg = df[df['method'] != 'Overall'].groupby(['task', 'method'], as_index=False).agg({\n",
        "    'normalized_march': 'mean', 'lower_is_better': 'first', 'sota': 'first'\n",
        "})\n",
        "methods = df_avg['method'].unique()\n",
        "\n",
        "# Order tasks by average march score\n",
        "task_order = sorted(df_avg['task'].unique(), key=lambda t: df_avg[df_avg['task']==t]['normalized_march'].mean(), reverse=True)\n",
        "task_to_num = {task: i+1 for i, task in enumerate(task_order)}\n",
        "df_avg['task_num'] = df_avg['task'].map(task_to_num)\n",
        "\n",
        "# Order methods by average score\n",
        "sorted_methods = df_avg.groupby('method')['normalized_march'].mean().sort_values(ascending=False).index.tolist()\n",
        "\n",
        "# Colors and shapes\n",
        "def parse_method_name(name):\n",
        "    parts = name.split('_', 1)\n",
        "    return (parts[0], parts[1]) if len(parts) == 2 else ('unknown', name)\n",
        "\n",
        "method_info = {m: dict(zip(['scaffold', 'model'], parse_method_name(m))) for m in methods}\n",
        "unique_models = sorted(set(info['model'] for info in method_info.values()))\n",
        "scaffold_to_models = defaultdict(list)\n",
        "for m, info in method_info.items():\n",
        "    scaffold_to_models[info['scaffold']].append(info['model'])\n",
        "\n",
        "scaffold_palettes = {\n",
        "    'greedy': sns.color_palette(\"Reds\", n_colors=max(3, len(set(scaffold_to_models['greedy']))+1))[1:],\n",
        "    'oneshot': sns.color_palette(\"Blues\", n_colors=max(3, len(set(scaffold_to_models['oneshot']))+1))[1:],\n",
        "}\n",
        "default_palette = sns.color_palette(\"Greens\", n_colors=2)[1:]\n",
        "\n",
        "method_to_color = {}\n",
        "for scaffold, models in scaffold_to_models.items():\n",
        "    palette = scaffold_palettes.get(scaffold, default_palette)\n",
        "    for i, model in enumerate(sorted(set(models))):\n",
        "        for m, info in method_info.items():\n",
        "            if info['scaffold'] == scaffold and info['model'] == model:\n",
        "                method_to_color[m] = palette[i % len(palette)]\n",
        "\n",
        "available_shapes = ['o', 's', '^', 'v', 'P', 'X', '*', 'h', 'H', 'p', '<', '>', '8', '+']\n",
        "model_to_shape = {model: available_shapes[i % len(available_shapes)] for i, model in enumerate(unique_models)}\n",
        "method_to_shape = {m: model_to_shape[info['model']] for m, info in method_info.items()}\n",
        "\n",
        "def format_method_label(name):\n",
        "    if '_' in name:\n",
        "        parts = name.split('_', 1)\n",
        "        scaffold = {'mlgym': 'ReAct', 'oneshot': 'One-Shot', 'greedy': 'Greedy'}.get(parts[0], parts[0])\n",
        "        model = parts[1].replace('gpt-4o', 'GPT-4o').replace('o3_mini', 'o3-mini').replace('cwm', 'CWM').replace('devstral', 'Devstral')\n",
        "        return f\"{scaffold} {model}\"\n",
        "    return name\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(28, 25))\n",
        "ax = plt.gca()\n",
        "ax.set_facecolor(\"#f0f5f0\")\n",
        "for spine in ax.spines.values():\n",
        "    spine.set_edgecolor('black')\n",
        "    spine.set_linewidth(2)\n",
        "\n",
        "for y in range(1, len(task_order)+1):\n",
        "    plt.axhline(y, color='gray', linestyle='--', linewidth=0.7, alpha=0.4, zorder=0)\n",
        "\n",
        "baseline_line = plt.axvline(0, color='red', linestyle='--', linewidth=2, alpha=0.7, zorder=1, label='Worst overall')\n",
        "sota_line = plt.axvline(1, color='black', linestyle='-', linewidth=2, alpha=0.7, zorder=1, label='SOTA')\n",
        "\n",
        "scatter_handles = []\n",
        "for method in sorted_methods:\n",
        "    group = df_avg[df_avg['method'] == method]\n",
        "    h = plt.scatter(group['normalized_march'], group['task_num'], label=method,\n",
        "                    color=method_to_color[method], marker=method_to_shape[method], s=350, alpha=0.85, zorder=2)\n",
        "    scatter_handles.append(h)\n",
        "\n",
        "method_handle_pairs = sorted(zip(sorted_methods, scatter_handles), key=lambda x: x[0].lower())\n",
        "handles = [h for _, h in method_handle_pairs] + [baseline_line, sota_line]\n",
        "labels = [format_method_label(m) for m, _ in method_handle_pairs] + ['Worst overall', 'SOTA']\n",
        "\n",
        "plt.legend(handles, labels, title='', fontsize=28, loc='lower right', bbox_to_anchor=(1, 0),\n",
        "           frameon=True, facecolor='#f0f5f0', edgecolor='black', framealpha=1.0)\n",
        "plt.yticks(range(1, len(task_order)+1), range(1, len(task_order)+1), fontsize=28)\n",
        "plt.xticks(np.arange(0, 2.1, 0.1), [f\"{x:.1f}\" for x in np.arange(0, 2.1, 0.1)], fontsize=36)\n",
        "plt.ylim(0.5, len(task_order)+0.5)\n",
        "plt.xlim(-0.01, 1.08)\n",
        "plt.gca().invert_yaxis()\n",
        "plt.xlabel('Normalized Score (March of 9s)', fontsize=32)\n",
        "plt.ylabel('Task Rank by Normalized Score', fontsize=32)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig(\"fig_march_of_9s_per_task.pdf\", bbox_inches=\"tight\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- March of 9's by Difficulty Band ---\n",
        "\n",
        "plt.figure(figsize=(32, 8))\n",
        "ax = plt.gca()\n",
        "ax.yaxis.grid(False)\n",
        "ax.xaxis.grid(True, which='major', linestyle='-', linewidth=1.6, color='#bbbbbb')\n",
        "bg_colors = ['#cbeed7', '#f3f3b7', '#ffd1c2', '#f7b6cd']\n",
        "difficulty_order = ['Easy', 'Medium', 'Hard', 'Expert']\n",
        "y_map = {label: i+1 for i, label in enumerate(difficulty_order)}\n",
        "\n",
        "# Divide tasks into 4 difficulty sections\n",
        "n_tasks = len(task_order)\n",
        "section_size = n_tasks // 4\n",
        "remainder = n_tasks % 4\n",
        "sizes = [section_size + (1 if i < remainder else 0) for i in range(4)]\n",
        "sections = {}\n",
        "category_task_ranks = {}\n",
        "start = 0\n",
        "for i, size in enumerate(sizes):\n",
        "    end = start + size\n",
        "    label = difficulty_order[i]\n",
        "    for task in task_order[start:end]:\n",
        "        sections[task] = label\n",
        "    category_task_ranks[label] = (start + 1, end) if task_order[start:end] else (None, None)\n",
        "    start = end\n",
        "df_avg['difficulty'] = df_avg['task'].map(sections)\n",
        "\n",
        "# Compute section averages\n",
        "task_method_avg = df_avg.groupby(['task', 'method'], as_index=False)['normalized_march'].mean()\n",
        "task_method_avg['difficulty'] = task_method_avg['task'].map(sections)\n",
        "section_avg = task_method_avg.groupby(['difficulty', 'method'], as_index=False)['normalized_march'].mean()\n",
        "\n",
        "# Scaffold averages\n",
        "scaffolds_to_show = ['greedy', 'mlgym', 'oneshot']\n",
        "scaffold_colors_display = {'greedy': 'red', 'oneshot': 'blue', 'mlgym': 'green'}\n",
        "scaffold_display_names = {'greedy': 'Greedy', 'mlgym': 'ReAct', 'oneshot': 'One-Shot'}\n",
        "prefix_section_avgs = {s: section_avg[section_avg['method'].str.startswith(s)].groupby('difficulty', as_index=False)['normalized_march'].mean() for s in scaffolds_to_show}\n",
        "overall_section_avg = task_method_avg.groupby('difficulty', as_index=False)['normalized_march'].mean()\n",
        "\n",
        "# Draw backgrounds\n",
        "for i, label in enumerate(difficulty_order):\n",
        "    ax.axhspan(y_map[label]-0.5, y_map[label]+0.5, color=bg_colors[i], alpha=0.4, zorder=0)\n",
        "\n",
        "# Plot method points\n",
        "for method in sorted(methods):\n",
        "    group = section_avg[section_avg['method'] == method]\n",
        "    y = [y_map[d] for d in group['difficulty']]\n",
        "    plt.scatter(group['normalized_march'], y, label=format_method_label(method),\n",
        "                color=method_to_color.get(method, 'gray'), marker=method_to_shape.get(method, 'o'),\n",
        "                s=360, alpha=0.8, zorder=2)\n",
        "\n",
        "# Add text annotations\n",
        "eps = 0.05\n",
        "plt.xlim(0.0-eps, 1.0+eps)\n",
        "right_x = ax.get_xlim()[1]\n",
        "x_text = right_x + 0.02\n",
        "avg_spacing = 0.18\n",
        "\n",
        "for i, label in enumerate(difficulty_order):\n",
        "    y_pos = y_map[label]\n",
        "    # Overall\n",
        "    overall_val = overall_section_avg[overall_section_avg['difficulty'] == label]['normalized_march']\n",
        "    if not overall_val.empty:\n",
        "        plt.text(x_text, y_pos + avg_spacing * 1.5, f\"overall: {overall_val.values[0]:.2f}\",\n",
        "                 fontsize=18, color='black', fontweight='bold', va='center', ha='left', clip_on=False)\n",
        "    # Scaffolds\n",
        "    for idx, scaffold in enumerate(['greedy', 'mlgym', 'oneshot']):\n",
        "        avg_val = prefix_section_avgs[scaffold][prefix_section_avgs[scaffold]['difficulty'] == label]['normalized_march']\n",
        "        if not avg_val.empty:\n",
        "            offset = avg_spacing * (0.5 - idx)\n",
        "            plt.text(x_text, y_pos + offset, f\"{scaffold_display_names[scaffold]}: {avg_val.values[0]:.2f}\",\n",
        "                     fontsize=18, color=scaffold_colors_display[scaffold], fontweight='bold', va='center', ha='left', clip_on=False)\n",
        "\n",
        "# Y-axis labels with task rank ranges\n",
        "ytick_labels = [f\"{label} ({category_task_ranks[label][0]} - {category_task_ranks[label][1]})\" if category_task_ranks[label][0] else f\"{label} (-)\" for label in difficulty_order]\n",
        "plt.yticks([y_map[l] for l in difficulty_order], ytick_labels, fontsize=22)\n",
        "for ticklabel in ax.get_yticklabels():\n",
        "    ticklabel.set_fontweight('bold')\n",
        "\n",
        "plt.xticks(np.arange(0.0, 1.1, 0.1), [f\"{x:.1f}\" for x in np.arange(0.0, 1.1, 0.1)], fontsize=27)\n",
        "plt.ylim(0.5, 4.5)\n",
        "ax.invert_yaxis()\n",
        "for spine in ax.spines.values():\n",
        "    spine.set_edgecolor('black')\n",
        "    spine.set_linewidth(2)\n",
        "\n",
        "plt.xlabel('Normalized Score (March of 9s)', fontsize=25)\n",
        "plt.ylabel('Task Difficulty', fontsize=25)\n",
        "plt.legend(title='Agent', fontsize=20, loc='center left', bbox_to_anchor=(1.10, 0.5),\n",
        "           frameon=True, facecolor='white', edgecolor='black', framealpha=1.0)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig(\"fig_march_of_9s_by_difficulty.pdf\", bbox_inches=\"tight\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. ANS Per Task (Linear)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Linear Normalized Score Scatter Plot (Per Task) ---\n",
        "\n",
        "# Linear normalization (identity function, no log transform)\n",
        "def linear_normalized(row):\n",
        "    if not row['is_valid'] or row['score'] is None or pd.isna(row['score']):\n",
        "        return 0.0\n",
        "    score, sota, worst = row['score'], row['sota'], row['worst_score']\n",
        "    denom = sota - worst\n",
        "    if abs(denom) < 1e-10:\n",
        "        return np.nan\n",
        "    normalized = (score - worst) / denom\n",
        "    return max(0, normalized)\n",
        "\n",
        "df['normalized_linear'] = df.apply(linear_normalized, axis=1)\n",
        "\n",
        "# Average per method per task\n",
        "df_avg_linear = df[df['method'] != 'Overall'].groupby(['task', 'method'], as_index=False).agg({\n",
        "    'normalized_linear': 'mean', 'lower_is_better': 'first', 'sota': 'first'\n",
        "})\n",
        "\n",
        "# Order tasks by average linear score\n",
        "task_order_linear = sorted(df_avg_linear['task'].unique(), key=lambda t: df_avg_linear[df_avg_linear['task']==t]['normalized_linear'].mean(), reverse=True)\n",
        "task_to_num_linear = {task: i+1 for i, task in enumerate(task_order_linear)}\n",
        "df_avg_linear['task_num'] = df_avg_linear['task'].map(task_to_num_linear)\n",
        "\n",
        "# Order methods by average score\n",
        "sorted_methods_linear = df_avg_linear.groupby('method')['normalized_linear'].mean().sort_values(ascending=False).index.tolist()\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(28, 25))\n",
        "ax = plt.gca()\n",
        "ax.set_facecolor(\"#fffff5\")\n",
        "for spine in ax.spines.values():\n",
        "    spine.set_edgecolor('black')\n",
        "    spine.set_linewidth(2)\n",
        "\n",
        "for y in range(1, len(task_order_linear)+1):\n",
        "    plt.axhline(y, color='gray', linestyle='--', linewidth=0.7, alpha=0.4, zorder=0)\n",
        "\n",
        "baseline_line = plt.axvline(0, color='red', linestyle='--', linewidth=2, alpha=0.7, zorder=1, label='Worst overall')\n",
        "sota_line = plt.axvline(1, color='black', linestyle='-', linewidth=2, alpha=0.7, zorder=1, label='SOTA')\n",
        "\n",
        "scatter_handles = []\n",
        "for method in sorted_methods_linear:\n",
        "    group = df_avg_linear[df_avg_linear['method'] == method]\n",
        "    h = plt.scatter(group['normalized_linear'], group['task_num'], label=method,\n",
        "                    color=method_to_color.get(method, 'gray'), marker=method_to_shape.get(method, 'o'),\n",
        "                    s=350, alpha=0.85, zorder=2)\n",
        "    scatter_handles.append(h)\n",
        "\n",
        "method_handle_pairs = sorted(zip(sorted_methods_linear, scatter_handles), key=lambda x: x[0].lower())\n",
        "handles = [h for _, h in method_handle_pairs] + [baseline_line, sota_line]\n",
        "labels = [format_method_label(m) for m, _ in method_handle_pairs] + ['Worst overall', 'SOTA']\n",
        "\n",
        "plt.legend(handles, labels, title='', fontsize=28, loc='lower right', bbox_to_anchor=(1, 0),\n",
        "           frameon=True, facecolor='#fffff5', edgecolor='black', framealpha=1.0)\n",
        "plt.yticks(range(1, len(task_order_linear)+1), range(1, len(task_order_linear)+1), fontsize=28)\n",
        "plt.xticks(np.arange(-0.1, 2.51, 0.1), [f\"{x:.1f}\" for x in np.arange(-0.1, 2.51, 0.1)], fontsize=36)\n",
        "plt.ylim(0.5, len(task_order_linear)+0.5)\n",
        "plt.xlim(-0.05, 1.05)\n",
        "plt.gca().invert_yaxis()\n",
        "plt.xlabel('Normalized Score (Linear)', fontsize=32)\n",
        "plt.ylabel('Task Rank by Normalized Score', fontsize=32)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig(\"fig_linear_scale_per_task.pdf\", bbox_inches=\"tight\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Linear Normalized Score by Difficulty Band ---\n",
        "\n",
        "# Assign difficulty bands based on linear task order\n",
        "sections_linear = {}\n",
        "category_task_ranks_linear = {}\n",
        "start = 0\n",
        "for i, size in enumerate(sizes):\n",
        "    end = start + size\n",
        "    label = difficulty_order[i]\n",
        "    for task in task_order_linear[start:end]:\n",
        "        sections_linear[task] = label\n",
        "    category_task_ranks_linear[label] = (start + 1, end) if task_order_linear[start:end] else (None, None)\n",
        "    start = end\n",
        "df_avg_linear['difficulty'] = df_avg_linear['task'].map(sections_linear)\n",
        "\n",
        "# Compute section averages\n",
        "task_method_avg_linear = df_avg_linear.groupby(['task', 'method'], as_index=False)['normalized_linear'].mean()\n",
        "task_method_avg_linear['difficulty'] = task_method_avg_linear['task'].map(sections_linear)\n",
        "section_avg_linear = task_method_avg_linear.groupby(['difficulty', 'method'], as_index=False)['normalized_linear'].mean()\n",
        "\n",
        "# Scaffold averages\n",
        "prefix_section_avgs_linear = {s: section_avg_linear[section_avg_linear['method'].str.startswith(s)].groupby('difficulty', as_index=False)['normalized_linear'].mean() for s in scaffolds_to_show}\n",
        "overall_section_avg_linear = task_method_avg_linear.groupby('difficulty', as_index=False)['normalized_linear'].mean()\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(32, 8))\n",
        "ax = plt.gca()\n",
        "ax.yaxis.grid(False)\n",
        "ax.xaxis.grid(True, which='major', linestyle='-', linewidth=1.6, color='#bbbbbb')\n",
        "\n",
        "# Draw backgrounds\n",
        "for i, label in enumerate(difficulty_order):\n",
        "    ax.axhspan(y_map[label]-0.5, y_map[label]+0.5, color=bg_colors[i], alpha=0.4, zorder=0)\n",
        "\n",
        "# Plot method points\n",
        "for method in sorted(methods):\n",
        "    group = section_avg_linear[section_avg_linear['method'] == method]\n",
        "    y = [y_map[d] for d in group['difficulty']]\n",
        "    plt.scatter(group['normalized_linear'], y, label=format_method_label(method),\n",
        "                color=method_to_color.get(method, 'gray'), marker=method_to_shape.get(method, 'o'),\n",
        "                s=360, alpha=0.8, zorder=2)\n",
        "\n",
        "# Add text annotations\n",
        "plt.xlim(0.0-eps, 1.0+eps)\n",
        "right_x = ax.get_xlim()[1]\n",
        "x_text = right_x + 0.02\n",
        "\n",
        "for i, label in enumerate(difficulty_order):\n",
        "    y_pos = y_map[label]\n",
        "    # Overall\n",
        "    overall_val = overall_section_avg_linear[overall_section_avg_linear['difficulty'] == label]['normalized_linear']\n",
        "    if not overall_val.empty:\n",
        "        plt.text(x_text, y_pos + avg_spacing * 1.5, f\"overall: {overall_val.values[0]:.2f}\",\n",
        "                 fontsize=18, color='black', fontweight='bold', va='center', ha='left', clip_on=False)\n",
        "    # Scaffolds\n",
        "    for idx, scaffold in enumerate(['greedy', 'mlgym', 'oneshot']):\n",
        "        avg_val = prefix_section_avgs_linear[scaffold][prefix_section_avgs_linear[scaffold]['difficulty'] == label]['normalized_linear']\n",
        "        if not avg_val.empty:\n",
        "            offset = avg_spacing * (0.5 - idx)\n",
        "            plt.text(x_text, y_pos + offset, f\"{scaffold_display_names[scaffold]}: {avg_val.values[0]:.2f}\",\n",
        "                     fontsize=18, color=scaffold_colors_display[scaffold], fontweight='bold', va='center', ha='left', clip_on=False)\n",
        "\n",
        "# Y-axis labels\n",
        "ytick_labels_linear = [f\"{label} ({category_task_ranks_linear[label][0]} - {category_task_ranks_linear[label][1]})\" if category_task_ranks_linear[label][0] else f\"{label} (-)\" for label in difficulty_order]\n",
        "plt.yticks([y_map[l] for l in difficulty_order], ytick_labels_linear, fontsize=22)\n",
        "for ticklabel in ax.get_yticklabels():\n",
        "    ticklabel.set_fontweight('bold')\n",
        "\n",
        "plt.xticks(np.arange(0.0, 1.1, 0.1), [f\"{x:.1f}\" for x in np.arange(0.0, 1.1, 0.1)], fontsize=27)\n",
        "plt.ylim(0.5, 4.5)\n",
        "ax.invert_yaxis()\n",
        "for spine in ax.spines.values():\n",
        "    spine.set_edgecolor('black')\n",
        "    spine.set_linewidth(2)\n",
        "\n",
        "plt.xlabel('Normalized Score (Linear)', fontsize=25)\n",
        "plt.ylabel('Task Difficulty', fontsize=25)\n",
        "plt.legend(title='Agent', fontsize=20, loc='center left', bbox_to_anchor=(1.10, 0.5),\n",
        "           frameon=True, facecolor='white', edgecolor='black', framealpha=1.0)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig(\"fig_linear_scale_by_difficulty.pdf\", bbox_inches=\"tight\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "fileHeader": "",
    "fileUid": "ded47475-ec41-46c8-89e4-117478af9ca1",
    "isAdHoc": false,
    "kernelspec": {
      "display_name": "albench (conda)",
      "language": "python",
      "name": "conda_albench"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
